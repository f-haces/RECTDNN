{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7621809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhacesga\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# NOTEBOOK IMPORTS\n",
    "import os, glob, zipfile, warnings\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from shutil import copyfile, rmtree\n",
    "from datetime import datetime\n",
    "\n",
    "# IMAGE IMPORTS\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# GIS IMPORTS\n",
    "import fiona, pyproj\n",
    "from affine import Affine\n",
    "from shapely.geometry import shape, mapping, Point, LineString, MultiPolygon\n",
    "from shapely.ops import transform, nearest_points, snap\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio.mask import mask\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# PLOTTING IMPORTS\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# CUSTOM UTILITIES\n",
    "from WorldFileUtils import *\n",
    "from GeometryUtils import *\n",
    "from icp import *\n",
    "from DataUtils import *\n",
    "from FindGrid import *\n",
    "from PlottingUtils import *\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = 933120000\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419ca032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGEOID(CID,):\n",
    "    # DEALING WITH A COMMUNITY ID (CID)\n",
    "    if CID >= 9e4:\n",
    "        output = CIDs[CIDs[\"CID\"] == CID][\"GEOID_p\"].to_numpy()\n",
    "    else: # DEALING WITH A COUNTY\n",
    "        output = np.asarray([CID])\n",
    "    \n",
    "    if output.size == 0:\n",
    "        return None\n",
    "    return output[0]\n",
    "\n",
    "def getGeometry(geoid, new_epsg=3857):\n",
    "    \n",
    "    project = pyproj.Transformer.from_crs(pyproj.CRS('EPSG:4326'), pyproj.CRS(f'EPSG:{new_epsg}'), \n",
    "                                          always_xy=True).transform\n",
    "    \n",
    "    # DEALING WITH A COMMUNITY ID (CID)\n",
    "    if geoid >= 9e4:\n",
    "        output = places[places[\"GEOID\"] == geoid][\"geometry\"].to_numpy()\n",
    "    else: # DEALING WITH A COUNTY\n",
    "        output = counties[counties[\"GEOID\"] == geoid][\"geometry\"].to_numpy()\n",
    "    if output.size == 0:\n",
    "        return None    \n",
    "    \n",
    "    output = transform(project, output[0])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def drawGrid(image_t, out):\n",
    "    # Create a blank image to draw the lines on\n",
    "    line_image = np.zeros_like(image_t)\n",
    "\n",
    "    for k, contours in out.items():\n",
    "        contours = contours.squeeze()\n",
    "        for i in range(contours.shape[0] - 1):\n",
    "            start_point = tuple(contours[i, :])\n",
    "            end_point = tuple(contours[i+1, :])\n",
    "            color = (255)  # You can change the color (BGR format) as needed\n",
    "            thickness = 10  # You can adjust the thickness of the line\n",
    "            line_image = cv2.line(line_image, start_point, end_point, color, thickness)\n",
    "            \n",
    "    return line_image > 0\n",
    "\n",
    "def adjustStep(from_points, coords_ras, kdtree, shear=True, rotation = True, perspective=True):\n",
    "    \n",
    "    # CALCULATE NEAREST POINTS AND FIND HOMOGRAPHY\n",
    "    _, nearest_indices = kdtree.query(from_points)\n",
    "    to_points = np.array([coords_ras[idx] for idx in nearest_indices])\n",
    "    new_homography, _ = cv2.findHomography(from_points, to_points, cv2.RANSAC, 10000)\n",
    "    if not shear:\n",
    "        scale  = np.sqrt((new_homography[0, 0] ** 2 + new_homography[1, 1] ** 2) / 2)\n",
    "        new_homography[0, 0] = scale \n",
    "        new_homography[1, 1] = scale\n",
    "    if not perspective:\n",
    "        new_homography[2, 0] = 0 \n",
    "        new_homography[2, 1] = 0 \n",
    "    if not rotation:\n",
    "        new_homography[0, 1] = 0 \n",
    "        new_homography[1, 0] = 0 \n",
    "    final_points = new_homography @ np.vstack((from_points[:, 0], from_points[:, 1], np.ones(from_points[:, 0].shape)))\n",
    "    \n",
    "    return final_points[:2, :].T, new_homography\n",
    "\n",
    "def find_bbox(binary_image):\n",
    "    # Find the coordinates of all \"True\" elements in the binary image\n",
    "    nonzero_points = cv2.findNonZero(binary_image)\n",
    "\n",
    "    if nonzero_points is None:\n",
    "        return None\n",
    "\n",
    "    # Calculate the bounding rectangle for the \"True\" elements\n",
    "    x, y, w, h = cv2.boundingRect(nonzero_points)\n",
    "\n",
    "    return np.array([x, y, x+w, y+h])\n",
    "\n",
    "def get_world_file_path(image_path):\n",
    "    # Get the file extension (e.g., \"png\", \"jpg\", \"tif\")\n",
    "    file_extension = image_path.split('.')[-1].lower()\n",
    "\n",
    "    # Define a dictionary to map file extensions to world file extensions\n",
    "    extension_mapping = {\n",
    "        'png': 'pgw',\n",
    "        'jpg': 'jpw',\n",
    "        'jpeg': 'jpw',  # You can add more extensions if needed\n",
    "        'tif': 'tfw',\n",
    "        'tiff': 'tfw',\n",
    "    }\n",
    "\n",
    "    # Check if the file extension is in the mapping\n",
    "    if file_extension in extension_mapping:\n",
    "        # Replace the file extension with the corresponding world file extension\n",
    "        world_file_extension = extension_mapping[file_extension]\n",
    "\n",
    "        # Create the world file path by replacing the image file extension with the world file extension\n",
    "        world_file_path = os.path.splitext(image_path)[0] + '.' + world_file_extension\n",
    "\n",
    "        return world_file_path\n",
    "    else:\n",
    "        return None  # Unsupported file extension\n",
    "    \n",
    "def plotICP(reprojected_points, plot_skip=2, ):\n",
    "    icp_iterations = len(reprojected_points)\n",
    "    fig, ax = plt.subplots()\n",
    "    colormap = plt.get_cmap('RdYlGn') \n",
    "\n",
    "    ax.scatter(coords_shp_proc[:, 0], coords_shp_proc[:, 1], color=colormap(0), s=0.5)\n",
    "    ax.scatter(coords_ras_proc[:, 0], coords_ras_proc[:, 1], color=\"black\", s=0.5)\n",
    "\n",
    "    for i in np.arange(plot_skip, icp_iterations, plot_skip):\n",
    "        ax.scatter(reprojected_points[i][:, 0], reprojected_points[i][:, 1], color=colormap(i / icp_iterations), s=0.1)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed09b63",
   "metadata": {},
   "source": [
    "IO dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34617015",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getlogin()\n",
    "\n",
    "if username == 'fhacesga':\n",
    "    base_input_path   = r\"D:\\FloodChange\\AAA_HistoricalDownload\"\n",
    "    base_output_path  = r\"C:\\Users\\\\\"+username+\"\\Desktop\\FIRMsDigitizing\\processing\"\n",
    "    ref_dir  = r\"C:\\Users\\fhacesga\\OneDrive - University Of Houston\\AAA_RECTDNN\\data\\AAA_ReferenceDatasets\\\\\"\n",
    "else:\n",
    "    base_input_path   = r\"D:\\Desktop\\FIRMsDigitizing\\data\\HistoricalFIRMS\"\n",
    "    base_output_path  = r\"D\\Desktop\\FIRMsDigitizing\\processing\"\n",
    "    ref_dir  = r\"C:\\Users\\franc\\OneDrive - University Of Houston\\AAA_RECTDNN\\data\\AAA_ReferenceDatasets\\\\\"\n",
    " \n",
    "CIDs     = pd.read_csv(f\"{ref_dir}CountyCIDs.csv\", index_col=0)\n",
    "counties = gpd.read_file(f\"{ref_dir}Counties.shp\")\n",
    "places   = gpd.read_file(f\"{ref_dir}Places.shp\")\n",
    "\n",
    "counties[\"GEOID\"] = counties[\"GEOID\"].astype(np.int32)\n",
    "places[\"GEOID\"]   = places[\"GEOID\"].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca1967",
   "metadata": {},
   "source": [
    "Create working dir and unzip all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca9abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_dir = r\"C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-09-08_16-35-58\"\n",
    "proc_dir = None\n",
    "# proc_dir = r\"C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-09-11_14-00-13\"\n",
    "proc_dir = r\"C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-09-20_16-29-07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb6d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if proc_dir is None:\n",
    "    datetime_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    proc_dir     = os.path.join(base_output_path, datetime_str)\n",
    "    unzipped_dir = os.path.join(proc_dir, \"Inputs\")\n",
    "    outputs_dir  = os.path.join(proc_dir, \"Outputs\")\n",
    "    print(proc_dir)\n",
    "    os.makedirs(proc_dir)\n",
    "    os.makedirs(unzipped_dir)\n",
    "    os.makedirs(outputs_dir)\n",
    "    extractZipFiles(base_input_path, unzipped_dir)\n",
    "else:\n",
    "    unzipped_dir = os.path.join(proc_dir, \"Inputs\")\n",
    "    outputs_dir  = os.path.join(proc_dir, \"Outputs\")\n",
    "    rmtree(outputs_dir)\n",
    "    os.makedirs(outputs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5180ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = glob.glob(f\"{unzipped_dir}/*\")\n",
    "filtered_files = [file for file in image_files if len(os.path.basename(file)) < 12]\n",
    "index_files = glob.glob(f\"{unzipped_dir}/*IND*\")\n",
    "\n",
    "index_files.extend(filtered_files)\n",
    "index_files = pd.DataFrame(index_files, columns=[\"FilePath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b89655",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_files[\"Basename\"] = [os.path.basename(file) for file in index_files[\"FilePath\"].to_list()]\n",
    "index_files[\"Location\"] = index_files[\"Basename\"].apply(extract_numerical_chars).astype(np.int32)\n",
    "index_files[\"GEOID\"] = index_files[\"Location\"].apply(getGEOID)\n",
    "index_files[\"geometry\"] = index_files[\"GEOID\"].apply(getGeometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde4b96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FilePath</th>\n",
       "      <th>Basename</th>\n",
       "      <th>Location</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...</td>\n",
       "      <td>480233IND0_0382.jpg</td>\n",
       "      <td>480233</td>\n",
       "      <td>4869908.0</td>\n",
       "      <td>(POLYGON ((-10637353.24214812 3452374.38747516...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...</td>\n",
       "      <td>480287IND0_0281.jpg</td>\n",
       "      <td>480287</td>\n",
       "      <td>48201.0</td>\n",
       "      <td>POLYGON ((-10670735.8423667 3487438.181082066,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...</td>\n",
       "      <td>480287IND0_0288.jpg</td>\n",
       "      <td>480287</td>\n",
       "      <td>48201.0</td>\n",
       "      <td>POLYGON ((-10670735.8423667 3487438.181082066,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...</td>\n",
       "      <td>480287IND0_0382.jpg</td>\n",
       "      <td>480287</td>\n",
       "      <td>48201.0</td>\n",
       "      <td>POLYGON ((-10670735.8423667 3487438.181082066,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...</td>\n",
       "      <td>480287IND0_0985.jpg</td>\n",
       "      <td>480287</td>\n",
       "      <td>48201.0</td>\n",
       "      <td>POLYGON ((-10670735.8423667 3487438.181082066,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...</td>\n",
       "      <td>485510B.tif</td>\n",
       "      <td>485510</td>\n",
       "      <td>4867688.0</td>\n",
       "      <td>POLYGON ((-10579345.43473218 3455313.758801992...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...</td>\n",
       "      <td>485513.tif</td>\n",
       "      <td>485513</td>\n",
       "      <td>4871960.0</td>\n",
       "      <td>POLYGON ((-10582980.01610658 3449596.838130131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...</td>\n",
       "      <td>485513A.tif</td>\n",
       "      <td>485513</td>\n",
       "      <td>4871960.0</td>\n",
       "      <td>POLYGON ((-10582980.01610658 3449596.838130131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...</td>\n",
       "      <td>485516.tif</td>\n",
       "      <td>485516</td>\n",
       "      <td>4876948.0</td>\n",
       "      <td>POLYGON ((-10591605.6068507 3445133.552947415,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...</td>\n",
       "      <td>485516A.tif</td>\n",
       "      <td>485516</td>\n",
       "      <td>4876948.0</td>\n",
       "      <td>POLYGON ((-10591605.6068507 3445133.552947415,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             FilePath             Basename  \\\n",
       "0   C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...  480233IND0_0382.jpg   \n",
       "1   C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...  480287IND0_0281.jpg   \n",
       "2   C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...  480287IND0_0288.jpg   \n",
       "3   C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...  480287IND0_0382.jpg   \n",
       "4   C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...  480287IND0_0985.jpg   \n",
       "..                                                ...                  ...   \n",
       "80  C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...          485510B.tif   \n",
       "81  C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...           485513.tif   \n",
       "82  C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...          485513A.tif   \n",
       "83  C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...           485516.tif   \n",
       "84  C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\proc...          485516A.tif   \n",
       "\n",
       "    Location      GEOID                                           geometry  \n",
       "0     480233  4869908.0  (POLYGON ((-10637353.24214812 3452374.38747516...  \n",
       "1     480287    48201.0  POLYGON ((-10670735.8423667 3487438.181082066,...  \n",
       "2     480287    48201.0  POLYGON ((-10670735.8423667 3487438.181082066,...  \n",
       "3     480287    48201.0  POLYGON ((-10670735.8423667 3487438.181082066,...  \n",
       "4     480287    48201.0  POLYGON ((-10670735.8423667 3487438.181082066,...  \n",
       "..       ...        ...                                                ...  \n",
       "80    485510  4867688.0  POLYGON ((-10579345.43473218 3455313.758801992...  \n",
       "81    485513  4871960.0  POLYGON ((-10582980.01610658 3449596.838130131...  \n",
       "82    485513  4871960.0  POLYGON ((-10582980.01610658 3449596.838130131...  \n",
       "83    485516  4876948.0  POLYGON ((-10591605.6068507 3445133.552947415,...  \n",
       "84    485516  4876948.0  POLYGON ((-10591605.6068507 3445133.552947415,...  \n",
       "\n",
       "[85 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2454f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindGrid(classifications, effectiveArea, key, image_path, verbose=True):\n",
    "\n",
    "    # Get largest section of mask image\n",
    "    image_or = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image_or, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (512, 512)) \n",
    "\n",
    "    a, b, c = identifyBiggestContour(effectiveArea[:, :, 1])\n",
    "    image_mask = cv2.drawContours(a[:, :, 0] * 0, contours=[c],contourIdx=-1, \n",
    "                                  color=(255), thickness=cv2.FILLED)\n",
    "    \n",
    "\n",
    "    # Detect lines\n",
    "    lines, result_image, scale_x, scale_y, thinimage = line_detection(classifications, effectiveArea, image)\n",
    "    writeImage(f\"tempfiles/{filename}_01_linedetection.png\", result_image, verbose)\n",
    "    writeImage(f\"tempfiles/{filename}_01_thinimage.png\", thinimage, verbose)\n",
    "\n",
    "    # FILTER BY MOST POPULAR ANGLES\n",
    "    angles = calcAngles(lines)\n",
    "    line_angles, line_indices, sorted_idx = filterLines_MostPopularAngles(np.array(angles), 0.5)\n",
    "    \n",
    "    # GET RESCALED LINES\n",
    "    rescaled_lines_ordered = np.array(lines)[sorted_idx]\n",
    "    filtered_lines = rescaled_lines_ordered[np.concatenate(line_indices).flatten()]\n",
    "    \n",
    "    if verbose:\n",
    "        plotLines(image_or, filtered_lines, savedir=f\"tempfiles/{filename}_02_azimuthfiltering.png\")\n",
    "\n",
    "    # Extend lines to edges and filter by distance between lines\n",
    "    extended_lines = extend_lines_to_edges(filtered_lines, image_or.shape)\n",
    "    if verbose:\n",
    "        plotLines(image_or, extended_lines, savedir=f\"tempfiles/{filename}_03_lineextension.png\")\n",
    "\n",
    "    # Filter lines by distance between endpoints and re-extend\n",
    "    min_distance = 50 * np.sqrt(scale_x ** 2 + scale_y ** 2)\n",
    "    filtered_lines, filtered_idx = filter_lines_by_distance(extended_lines, min_distance)\n",
    "    extended_lines = extend_lines_to_edges(filtered_lines, image_or.shape)\n",
    "    if verbose:\n",
    "        plotLines(image_or, extended_lines, savedir=f\"tempfiles/{filename}_04_distancefiltering.png\")\n",
    "\n",
    "    # Clip lines\n",
    "    lines_shp   = lines_to_linestrings(filtered_lines)\n",
    "    split_lines = linestrings_to_lines(unary_union(lines_shp))\n",
    "    if verbose:\n",
    "        plotLines(image_or, extended_lines, savedir=f\"tempfiles/{filename}_05_lineclipping.png\")\n",
    "\n",
    "    overlapping_lines, overlap_values = get_overlapping_lines(split_lines, \n",
    "                                                             thinimage, \n",
    "                                                             0.8,\n",
    "                                                             verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        plotLines(image_or, overlapping_lines, savedir=f\"tempfiles/{filename}_06_overlappinglines.png\")\n",
    "    \n",
    "    # Convert lines to an image in which we identify contours\n",
    "    bw_bounds = draw_lines_to_image(overlapping_lines, (image_or.shape[1], image_or.shape[0]))\n",
    "    contours, hierarchy = cv2.findContours(bw_bounds, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
    "    writeImage(f\"tempfiles/{filename}_06_drawnimage.png\", bw_bounds, verbose)\n",
    "    highest_level = np.max(hierarchy, axis=1).flatten()[3]\n",
    "    print(f\"Highest Hierarchy: {highest_level} in {hierarchy.shape[1]} contours\")\n",
    "    \n",
    "    # Test which squares are identified\n",
    "    if verbose:\n",
    "        filled_image = np.zeros(image_or.shape)\n",
    "        \n",
    "        # Fill innermost contours with random colors\n",
    "        for idx, contour in enumerate(contours):\n",
    "            \n",
    "            if hierarchy[0][idx][3] == highest_level:  # If contour has no child contours\n",
    "                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "                cv2.drawContours(filled_image, [contour], -1, color, thickness=cv2.FILLED)\n",
    "        writeImage(f\"tempfiles/{filename}_06_recognizedsquares.png\", filled_image, verbose)\n",
    "\n",
    "\n",
    "    # Find which squares have a given text\n",
    "    outdict = {}\n",
    "\n",
    "    \n",
    "\n",
    "    for idx, contour in tqdm(enumerate(contours), total=len(contours), disable=~verbose):\n",
    "        if hierarchy[0][idx][3] == highest_level:  # If contour has no child contours\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "            # Crop the contour region from the image\n",
    "            cropped_region = image_or[y:y+h, x:x+w, 0]\n",
    "            cropped_region = pad_image_with_percentage(cropped_region, 20, 20)\n",
    "            \n",
    "            writeImage(f\"tempfiles/{filename}_07_{idx}.png\", cropped_region, verbose)\n",
    "            \n",
    "            # Perform OCR using pytesseract\n",
    "            ocr_text = pytesseract.image_to_string(cropped_region,\n",
    "                                                  config='--psm 12 --oem 3')\n",
    "                                                  # -c tessedit_char_whitelist=0123456789\n",
    "            \n",
    "            if len(ocr_text) == 0:\n",
    "                continue\n",
    "                \n",
    "            text = find_word_with_key(ocr_text, key, verbose=verbose)\n",
    "            \n",
    "            if text is None:\n",
    "                continue\n",
    "            \n",
    "            if isinstance(text, list):\n",
    "                print(\"Found too many names! Splitting along longest sides\")\n",
    "                try:\n",
    "                    shapely_contour = contours_to_shapely_polygons(contour)\n",
    "                    outpoly_1, outpoly_2 = splitPolygonByLongerSides(shapely_contour)\n",
    "                    \n",
    "                    outdict[text[0]] = convertShapelyToCV2(outpoly_1)\n",
    "                    outdict[text[1]] = convertShapelyToCV2(outpoly_2)\n",
    "                except:\n",
    "                    print(\"Failure! Results will be inaccurate due to line segment on Tile Boundary not being identified\")\n",
    "                    continue\n",
    "                continue\n",
    "            outdict[text] = contour\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    return outdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbac5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-09-20_16-29-07\\Inputs\\480233IND0_0382.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8d7871166c4fbb98c7b440ffeb7e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-09-20_16-29-07\\Inputs\\480287IND0_0281.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e8db01f0ac4ad4a0a2f2268fe143e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-09-20_16-29-07\\Inputs\\480287IND0_0288.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a63564eea44571bde2c94e8be17d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-09-20_16-29-07\\Inputs\\480287IND0_0382.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b47d61818b4ce2bd013228edf73f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "verbose = True\n",
    "\n",
    "for i, row in index_files.iterrows():\n",
    "    print(row[\"FilePath\"])\n",
    "    \n",
    "    filename = os.path.basename(row[\"FilePath\"])\n",
    "    \n",
    "    # READ FILES AND RUN CLASSIFICATIONS\n",
    "    image = cv2.imread(row[\"FilePath\"])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # SAVE IMAGE TO OUTPUT DIRECTORY\n",
    "    output_image = os.path.join(outputs_dir, filename)\n",
    "    copyfile(row[\"FilePath\"], output_image)\n",
    "    \n",
    "    # GET WHICHEVER POLYGON IS THE LARGEST IN THE ROW'S GEOMETRY AND SIMPLIFY\n",
    "    if row[\"geometry\"] is None:\n",
    "        continue\n",
    "    elif isinstance(row[\"geometry\"], MultiPolygon):\n",
    "        largest_polygon_area = np.argmax([a.area for a in row[\"geometry\"]])\n",
    "        largest_polygon = row[\"geometry\"][largest_polygon_area].simplify(tolerance=20)\n",
    "        largest_polygon = largest_polygon.boundary\n",
    "    else:\n",
    "        largest_polygon = row[\"geometry\"].boundary\n",
    "    \n",
    "    # CONVERT POLYGON TO POINTS\n",
    "    length = largest_polygon.length # POLYGON LENGTH\n",
    "    point_boundary_list = list()    # OUTPUT POINT LIST\n",
    "    \n",
    "    # INTERPOLATE THROUGH ALL OF LENGTH\n",
    "    for distance in tqdm(range(0,int(length),20), disable=True):         \n",
    "        point = largest_polygon.interpolate(distance)   \n",
    "        point_boundary_list.append(point)\n",
    "    point_boundary_gdf = gpd.GeoDataFrame(geometry=point_boundary_list)\n",
    "    \n",
    "    key = findKey(row[\"Basename\"])    \n",
    "    if key is None:\n",
    "        print(f\"Could not find key in {filename}\")\n",
    "        \n",
    "    # RUN IMAGES THROUGH CNNs\n",
    "    classifications, classModel       = findKeypoints(image)\n",
    "    effectiveArea, effectiveAreaModel = findSquares(image)\n",
    "    cv2.imwrite(f\"tempfiles/{filename}_00_classification.png\", np.asarray(probability_to_rgb(classifications)))\n",
    "    cv2.imwrite(f\"tempfiles/{filename}_00_effectiveArea.png\", np.asarray(probability_to_rgb(effectiveArea)))\n",
    "    # writeImage(f\"tempfiles/{filename}_00_classification.png\", probability_to_rgb(classifications), verbose)\n",
    "    # writeImage(f\"tempfiles/{filename}_00_effectiveArea.png\", probability_to_rgb(effectiveArea), verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01ee64fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f39579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3778, 3780,    3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((classifications.shape[:-1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9cfcb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3778, 3780, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6b30fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(f\"tempfiles/{filename}_00_classification.png\", probability_to_rgb(classifications))\n",
    "plt.imsave(f\"tempfiles/{filename}_00_effectiveArea.png\", probability_to_rgb(effectiveArea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8d752",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "    # FIND THE GRID INSIDE THE IMAGE\n",
    "    out = FindGrid(row[\"FilePath\"], verbose=False)\n",
    "\n",
    "    # PLOT IMAGE\n",
    "    plot_image = np.dstack([image, image, image])\n",
    "    contours = []\n",
    "    \n",
    "    \n",
    "    # LOOP THROUGH CONTOURS AND ADD THEM TO IMAGE\n",
    "    for k, v in out.items():\n",
    "        cv2.drawContours(plot_image, v, -1, (0, 255, 0), 3)\n",
    "        contours.append(contours_to_shapely_polygons(v))\n",
    "        \n",
    "    # CALCULATE BOUNDS OF CONTOURS ON IMAGE\n",
    "    bounds_panels = MultiPolygon(contours).bounds\n",
    "    bounds_panels = [int(i) for i in bounds_panels]\n",
    "\n",
    "    # CREATE MASK USING GRID \n",
    "    mask = np.zeros(image.shape)\n",
    "    mask[bounds_panels[1]+50:bounds_panels[3]-50, bounds_panels[0]+50:bounds_panels[2]-50] = 1\n",
    "    mask_1 = drawGrid(image, out)          # GRID MASKING\n",
    "    mask = np.logical_and(~mask_1, mask)   # MERGE MASKS\n",
    "    \n",
    "    # FLIP IMAGE AND MASK IT\n",
    "    image = 255 - image\n",
    "    image_t = image * mask\n",
    "    \n",
    "    # RASTER BOUNDS\n",
    "    get_bounds_image = np.where(np.asarray(cv2.erode(image_t, np.ones((3,3), np.uint8)) > 50), 1, 0)\n",
    "    # plt.imshow(get_bounds_image)\n",
    "    # plt.show()\n",
    "    \n",
    "    # print(get_bounds_image)\n",
    "    bounds_panels_postfilter = find_bbox(get_bounds_image)\n",
    "    bounds_panels = [i for i in bounds_panels_postfilter]\n",
    "                                  \n",
    "    # SHAPEFILE BOUNDS\n",
    "    shp_bounds = [i for i in largest_polygon.bounds]\n",
    "\n",
    "    print(\"Assigning Points\")\n",
    "    # INITIAL TRANSFORM\n",
    "    from_points   = np.array([[bounds_panels[0], bounds_panels[1]], \n",
    "                   [bounds_panels[0], bounds_panels[3]],\n",
    "                   [bounds_panels[2], bounds_panels[1]],\n",
    "                   [bounds_panels[2], bounds_panels[3]]])\n",
    "\n",
    "    to_points = np.array([[shp_bounds[0], shp_bounds[1]], \n",
    "                   [shp_bounds[0], shp_bounds[3]],\n",
    "                   [shp_bounds[2], shp_bounds[1]],\n",
    "                   [shp_bounds[2], shp_bounds[3]]])\n",
    "\n",
    "    initial_transform = cv2.findHomography(from_points, to_points, cv2.RANSAC, 1000)\n",
    "    original_homography = initial_transform[0]\n",
    "    inverse_transform = np.linalg.inv(original_homography)\n",
    "\n",
    "    # CONVERT THINNED IMAGE TO POINTS\n",
    "    thin_image = cv2.ximgproc.thinning(image_t.astype(np.uint8), thinningType=cv2.ximgproc.THINNING_GUOHALL)\n",
    "    y, x = np.where(thin_image[::-1, :])                   # GET COORDINATES OF EVERY \n",
    "    image_points = np.vstack((x, y, np.ones(x.shape)))     # STACK X, Y, AND Z COORDINATES\n",
    "    # outputs = original_homography @ image_points           # TRANSFORM COORDINATES USING ESTIMATES\n",
    "    \n",
    "    # TRANSFORM SHAPEFILE POINTS INTO IMAGE COORDINATE SYSTEM\n",
    "    point_geometry = [[point.geometry.x, point.geometry.y, 1] for i, point in point_boundary_gdf.iterrows()]\n",
    "    point_geometry = inverse_transform @ np.array(point_geometry).T\n",
    "    \n",
    "    # COORDINATE HANDLING\n",
    "    coords_shp = point_geometry.T\n",
    "    coords_ras = np.vstack((image_points[0, :], image_points[1, :], np.ones(image_points[1, :].shape))).T\n",
    "    \n",
    "    # IMAGE ORIGIN COORDINATE SYSTEM TO IMAGE CENTER COORDINATE SYSTEM\n",
    "    offsets = np.mean(coords_ras, axis=0)\n",
    "    x_offset = offsets[0]\n",
    "    y_offset = offsets[1]\n",
    "\n",
    "    coords_shp_proc = np.vstack((coords_shp[:, 0] - x_offset, coords_shp[:, 1] - y_offset)).T\n",
    "    coords_ras_proc = np.vstack((coords_ras[:, 0] - x_offset, coords_ras[:, 1] - y_offset)).T\n",
    "    \n",
    "    # FAST SEARCH STRUCTURE\n",
    "    kdtree = cKDTree(coords_ras_proc)\n",
    "    \n",
    "    # ITERATIVE CLOSEST POINT\n",
    "    reprojected_points = []\n",
    "    compounded_homography = np.eye(3)\n",
    "    proc_points = coords_shp_proc\n",
    "    \n",
    "    # ITERATE\n",
    "    for i in tqdm(range(20), disable=True):\n",
    "        # TAKE ADJUSTMENT STEP\n",
    "        out, new_homography = adjustStep(proc_points, coords_ras_proc, kdtree,\n",
    "                                        shear=False, rotation=False, perspective=False)\n",
    "        compounded_homography = compounded_homography @ new_homography\n",
    "        reprojected_points.append(out)\n",
    "        proc_points = out\n",
    "        if i % 10 == 0:\n",
    "            scale  = np.sqrt((new_homography[0, 0] ** 2 + new_homography[1, 1] ** 2) / 2)\n",
    "            offset = np.sqrt((new_homography[1, 2] ** 2 + new_homography[0, 2] ** 2) / 2)\n",
    "\n",
    "            print(f\"Scale: {scale:.2f} Offset: {offset:.2f}\")\n",
    "    \n",
    "    plotICP(reprojected_points, plot_skip=10, )\n",
    "    plt.show()\n",
    "    \n",
    "    # REVERSE Y AXIS\n",
    "    rev_y_axis = np.array([[1, 0, 0],\n",
    "                           [0,-1, 0],\n",
    "                           [0, 0, 1]])\n",
    "\n",
    "    # move = original_homography @ np.array([0, image_t.shape[0], 0])\n",
    "    translation = np.eye(3)\n",
    "    translation[1, 2] = image_t.shape[0]\n",
    "\n",
    "    adjustment = np.linalg.inv(compounded_homography.copy())\n",
    "    adjustment[0, 2] = -1 * adjustment[0, 2]\n",
    "    adjustment[1, 2] = -1 * adjustment[1, 2]\n",
    "    # temp = adjustment[0, 2]\n",
    "    # adjustment[0, 2] = adjustment[1, 2]\n",
    "    # adjustment[1, 2] = temp\n",
    "    output_transform = original_homography @ adjustment @ translation @ rev_y_axis \n",
    "    print(output_transform)\n",
    "    # output_transform = original_homography @ translation @ rev_y_axis \n",
    "      \n",
    "    output_affine = Affine(*output_transform.flatten()[:6])\n",
    "    write_world_file_from_affine(output_affine, get_world_file_path(output_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820645c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
