{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7621809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK IMPORTS\n",
    "import os, glob, zipfile, warnings\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from shutil import copyfile, rmtree\n",
    "from datetime import datetime\n",
    "\n",
    "# IMAGE IMPORTS\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# GIS IMPORTS\n",
    "import fiona, pyproj\n",
    "from affine import Affine\n",
    "from shapely.geometry import shape, mapping, Point, LineString, MultiPolygon\n",
    "from shapely.ops import transform, nearest_points, snap\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio.mask import mask\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# PLOTTING IMPORTS\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# CUSTOM UTILITIES\n",
    "from WorldFileUtils import *\n",
    "from GeometryUtils import *\n",
    "from icp import *\n",
    "from DataUtils import *\n",
    "from FindGrid import *\n",
    "from PlottingUtils import *\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = 933120000\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419ca032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGEOID(CID,):\n",
    "    # DEALING WITH A COMMUNITY ID (CID)\n",
    "    if CID >= 9e4:\n",
    "        output = CIDs[CIDs[\"CID\"] == CID][\"GEOID_p\"].to_numpy()\n",
    "    else: # DEALING WITH A COUNTY\n",
    "        output = np.asarray([CID])\n",
    "    \n",
    "    if output.size == 0:\n",
    "        return None\n",
    "    return output[0]\n",
    "\n",
    "def getGeometry(geoid, new_epsg=3857):\n",
    "    \n",
    "    project = pyproj.Transformer.from_crs(pyproj.CRS('EPSG:4326'), pyproj.CRS(f'EPSG:{new_epsg}'), \n",
    "                                          always_xy=True).transform\n",
    "    \n",
    "    # DEALING WITH A COMMUNITY ID (CID)\n",
    "    if geoid >= 9e4:\n",
    "        output = places[places[\"GEOID\"] == geoid][\"geometry\"].to_numpy()\n",
    "    else: # DEALING WITH A COUNTY\n",
    "        output = counties[counties[\"GEOID\"] == geoid][\"geometry\"].to_numpy()\n",
    "    if output.size == 0:\n",
    "        return None    \n",
    "    \n",
    "    output = transform(project, output[0])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def drawGrid(image_t, out):\n",
    "    # Create a blank image to draw the lines on\n",
    "    line_image = np.zeros_like(image_t)\n",
    "\n",
    "    for k, contours in out.items():\n",
    "        contours = contours.squeeze()\n",
    "        for i in range(contours.shape[0] - 1):\n",
    "            start_point = tuple(contours[i, :])\n",
    "            end_point = tuple(contours[i+1, :])\n",
    "            color = (255)  # You can change the color (BGR format) as needed\n",
    "            thickness = 10  # You can adjust the thickness of the line\n",
    "            line_image = cv2.line(line_image, start_point, end_point, color, thickness)\n",
    "            \n",
    "    return line_image > 0\n",
    "\n",
    "def adjustStep(from_points, coords_ras, kdtree, shear=True, rotation = True, perspective=True):\n",
    "    \n",
    "    # CALCULATE NEAREST POINTS AND FIND HOMOGRAPHY\n",
    "    _, nearest_indices = kdtree.query(from_points)\n",
    "    to_points = np.array([coords_ras[idx] for idx in nearest_indices])\n",
    "    new_homography, _ = cv2.findHomography(from_points, to_points, cv2.RANSAC, 10000)\n",
    "    if not shear:\n",
    "        scale  = np.sqrt((new_homography[0, 0] ** 2 + new_homography[1, 1] ** 2) / 2)\n",
    "        new_homography[0, 0] = scale \n",
    "        new_homography[1, 1] = scale\n",
    "    if not perspective:\n",
    "        new_homography[2, 0] = 0 \n",
    "        new_homography[2, 1] = 0 \n",
    "    if not rotation:\n",
    "        new_homography[0, 1] = 0 \n",
    "        new_homography[1, 0] = 0 \n",
    "    final_points = new_homography @ np.vstack((from_points[:, 0], from_points[:, 1], np.ones(from_points[:, 0].shape)))\n",
    "    \n",
    "    return final_points[:2, :].T, new_homography\n",
    "\n",
    "def find_bbox(binary_image):\n",
    "    # Find the coordinates of all \"True\" elements in the binary image\n",
    "    nonzero_points = cv2.findNonZero(binary_image)\n",
    "\n",
    "    if nonzero_points is None:\n",
    "        return None\n",
    "\n",
    "    # Calculate the bounding rectangle for the \"True\" elements\n",
    "    x, y, w, h = cv2.boundingRect(nonzero_points)\n",
    "\n",
    "    return np.array([x, y, x+w, y+h])\n",
    "\n",
    "def get_world_file_path(image_path):\n",
    "    # Get the file extension (e.g., \"png\", \"jpg\", \"tif\")\n",
    "    file_extension = image_path.split('.')[-1].lower()\n",
    "\n",
    "    # Define a dictionary to map file extensions to world file extensions\n",
    "    extension_mapping = {\n",
    "        'png': 'pgw',\n",
    "        'jpg': 'jpw',\n",
    "        'jpeg': 'jpw',  # You can add more extensions if needed\n",
    "        'tif': 'tfw',\n",
    "        'tiff': 'tfw',\n",
    "    }\n",
    "\n",
    "    # Check if the file extension is in the mapping\n",
    "    if file_extension in extension_mapping:\n",
    "        # Replace the file extension with the corresponding world file extension\n",
    "        world_file_extension = extension_mapping[file_extension]\n",
    "\n",
    "        # Create the world file path by replacing the image file extension with the world file extension\n",
    "        world_file_path = os.path.splitext(image_path)[0] + '.' + world_file_extension\n",
    "\n",
    "        return world_file_path\n",
    "    else:\n",
    "        return None  # Unsupported file extension\n",
    "    \n",
    "def plotICP(reprojected_points, plot_skip=2, ):\n",
    "    icp_iterations = len(reprojected_points)\n",
    "    fig, ax = plt.subplots()\n",
    "    colormap = plt.get_cmap('RdYlGn') \n",
    "\n",
    "    ax.scatter(coords_shp_proc[:, 0], coords_shp_proc[:, 1], color=colormap(0), s=0.5)\n",
    "    ax.scatter(coords_ras_proc[:, 0], coords_ras_proc[:, 1], color=\"black\", s=0.5)\n",
    "\n",
    "    for i in np.arange(plot_skip, icp_iterations, plot_skip):\n",
    "        ax.scatter(reprojected_points[i][:, 0], reprojected_points[i][:, 1], color=colormap(i / icp_iterations), s=0.1)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed09b63",
   "metadata": {},
   "source": [
    "IO dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34617015",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getlogin()\n",
    "\n",
    "if username == 'fhacesga':\n",
    "    base_input_path   = r\"D:\\FloodChange\\AAA_HistoricalDownload\"\n",
    "    base_output_path  = r\"C:\\Users\\\\\"+username+\"\\Desktop\\FIRMsDigitizing\\processing\"\n",
    "    ref_dir  = r\"C:\\Users\\fhacesga\\OneDrive - University Of Houston\\AAA_RECTDNN\\data\\AAA_ReferenceDatasets\\\\\"\n",
    "if username == 'fhace':\n",
    "    ref_dir = r\"C:\\Users\\fhace\\OneDrive - University Of Houston\\AAA_RECTDNN\\data\\AAA_ReferenceDatasets\\\\\"\n",
    "else:\n",
    "    base_input_path   = r\"D:\\Desktop\\FIRMsDigitizing\\data\\HistoricalFIRMS\"\n",
    "    base_output_path  = r\"D\\Desktop\\FIRMsDigitizing\\processing\"\n",
    "    ref_dir  = r\"C:\\Users\\franc\\OneDrive - University Of Houston\\AAA_RECTDNN\\data\\AAA_ReferenceDatasets\\\\\"\n",
    "\n",
    "CIDs     = pd.read_csv(f\"{ref_dir}CountyCIDs.csv\", index_col=0)\n",
    "counties = gpd.read_file(f\"{ref_dir}Counties.shp\")\n",
    "places   = gpd.read_file(f\"{ref_dir}Places.shp\")\n",
    "\n",
    "counties[\"GEOID\"] = counties[\"GEOID\"].astype(np.int32)\n",
    "places[\"GEOID\"]   = places[\"GEOID\"].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca1967",
   "metadata": {},
   "source": [
    "Create working dir and unzip all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca9abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_dir = r\"C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-09-08_16-35-58\"\n",
    "proc_dir = None\n",
    "# proc_dir = r\"C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-09-11_14-00-13\"\n",
    "proc_dir = r\"C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-09-20_16-29-07\"\n",
    "proc_dir = r\"C:\\Users\\fhace\\OneDrive - University Of Houston\\AAA_RECTDNN\\processing\\2023-09-08_11-24-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb6d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if proc_dir is None:\n",
    "    datetime_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    proc_dir     = os.path.join(base_output_path, datetime_str)\n",
    "    unzipped_dir = os.path.join(proc_dir, \"Inputs\")\n",
    "    outputs_dir  = os.path.join(proc_dir, \"Outputs\")\n",
    "    print(proc_dir)\n",
    "    os.makedirs(proc_dir)\n",
    "    os.makedirs(unzipped_dir)\n",
    "    os.makedirs(outputs_dir)\n",
    "    extractZipFiles(base_input_path, unzipped_dir)\n",
    "else:\n",
    "    unzipped_dir = os.path.join(proc_dir, \"Inputs\")\n",
    "    outputs_dir  = os.path.join(proc_dir, \"Outputs\")\n",
    "    rmtree(outputs_dir)\n",
    "    os.makedirs(outputs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5180ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = glob.glob(f\"{unzipped_dir}/*\")\n",
    "filtered_files = [file for file in image_files if len(os.path.basename(file)) < 12]\n",
    "index_files = glob.glob(f\"{unzipped_dir}/*IND*\")\n",
    "\n",
    "index_files.extend(filtered_files)\n",
    "index_files = pd.DataFrame(index_files, columns=[\"FilePath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b89655",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_files[\"Basename\"] = [os.path.basename(file) for file in index_files[\"FilePath\"].to_list()]\n",
    "index_files[\"Location\"] = index_files[\"Basename\"].apply(extract_numerical_chars).astype(np.int32)\n",
    "index_files[\"GEOID\"] = index_files[\"Location\"].apply(getGEOID)\n",
    "index_files[\"geometry\"] = index_files[\"GEOID\"].apply(getGeometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed2454f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindGrid(image, classifications, effectiveArea, key, image_path, verbose=True):    \n",
    "    \n",
    "    # Detect lines\n",
    "    lines, result_image, scale_x, scale_y, thinimage = line_detection(classifications, effectiveArea, image)\n",
    "    writeImage(f\"tempfiles/{filename}_01_linedetection.png\", result_image, verbose)\n",
    "    writeImage(f\"tempfiles/{filename}_01_thinimage.png\", thinimage, verbose)\n",
    "\n",
    "    # FILTER BY MOST POPULAR ANGLES\n",
    "    angles = calcAngles(lines)\n",
    "    line_angles, line_indices, sorted_idx = filterLines_MostPopularAngles(np.array(angles), 0.5)\n",
    "    \n",
    "    # GET RESCALED LINES\n",
    "    rescaled_lines_ordered = np.array(lines)[sorted_idx]\n",
    "    filtered_lines = rescaled_lines_ordered[np.concatenate(line_indices).flatten()]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"saving\")\n",
    "        plotLines(image, filtered_lines, savedir=f\"tempfiles/{filename}_02_azimuthfiltering.png\")\n",
    "\n",
    "    # Extend lines to edges and filter by distance between lines\n",
    "    extended_lines = extend_lines_to_edges(filtered_lines, image.shape)\n",
    "    if verbose:\n",
    "        plotLines(image, extended_lines, savedir=f\"tempfiles/{filename}_03_lineextension.png\")\n",
    "\n",
    "    # Filter lines by distance between endpoints and re-extend\n",
    "    min_distance = 50 * np.sqrt(scale_x ** 2 + scale_y ** 2)\n",
    "    filtered_lines, filtered_idx = filter_lines_by_distance(extended_lines, min_distance)\n",
    "    extended_lines = extend_lines_to_edges(filtered_lines, image.shape)\n",
    "    if verbose:\n",
    "        plotLines(image, extended_lines, savedir=f\"tempfiles/{filename}_04_distancefiltering.png\")\n",
    "\n",
    "    # Clip lines\n",
    "    lines_shp   = lines_to_linestrings(filtered_lines)\n",
    "    split_lines = linestrings_to_lines(unary_union(lines_shp))\n",
    "    if verbose:\n",
    "        plotLines(image, extended_lines, savedir=f\"tempfiles/{filename}_05_lineclipping.png\")\n",
    "\n",
    "    overlapping_lines, overlap_values = get_overlapping_lines(split_lines, \n",
    "                                                             thinimage, \n",
    "                                                             0.8,\n",
    "                                                             verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        plotLines(image, overlapping_lines, savedir=f\"tempfiles/{filename}_06_overlappinglines.png\")\n",
    "    \n",
    "    # Convert lines to an image in which we identify contours\n",
    "    bw_bounds = draw_lines_to_image(overlapping_lines, (image.shape[1], image.shape[0]))\n",
    "    contours, hierarchy = cv2.findContours(bw_bounds, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
    "    writeImage(f\"tempfiles/{filename}_06_drawnimage.png\", bw_bounds, verbose)\n",
    "    highest_level = np.max(hierarchy, axis=1).flatten()[3]\n",
    "    print(f\"Highest Hierarchy: {highest_level} in {hierarchy.shape[1]} contours\")\n",
    "    \n",
    "    # Test which squares are identified\n",
    "    if verbose:\n",
    "        filled_image = np.zeros(image.shape)\n",
    "        \n",
    "        # Fill innermost contours with random colors\n",
    "        for idx, contour in enumerate(contours):\n",
    "            \n",
    "            if hierarchy[0][idx][3] == highest_level:  # If contour has no child contours\n",
    "                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "                cv2.drawContours(filled_image, [contour], -1, color, thickness=cv2.FILLED)\n",
    "        writeImage(f\"tempfiles/{filename}_06_recognizedsquares.png\", filled_image, verbose)\n",
    "\n",
    "\n",
    "    # Find which squares have a given text\n",
    "    outdict = {}\n",
    "    for idx, contour in tqdm(enumerate(contours), total=len(contours), disable=~verbose):\n",
    "        if hierarchy[0][idx][3] == highest_level:  # If contour has no child contours\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "            # Crop the contour region from the image\n",
    "            if image.ndim == 3:\n",
    "                cropped_region = image[y:y+h, x:x+w, 0]\n",
    "            else: \n",
    "                cropped_region = image[y:y+h, x:x+w]\n",
    "            cropped_region = pad_image_with_percentage(cropped_region, 20, 20)\n",
    "            \n",
    "            writeImage(f\"tempfiles/{filename}_07_{idx}.png\", cropped_region, verbose)\n",
    "            \n",
    "            # Perform OCR using pytesseract\n",
    "            ocr_text = pytesseract.image_to_string(cropped_region,\n",
    "                                                  config='--psm 12 --oem 3')\n",
    "                                                  # -c tessedit_char_whitelist=0123456789\n",
    "            \n",
    "            if len(ocr_text) == 0:\n",
    "                continue\n",
    "                \n",
    "            text = find_word_with_key(ocr_text, key, verbose=verbose)\n",
    "            \n",
    "            if text is None:\n",
    "                continue\n",
    "            \n",
    "            if isinstance(text, list):\n",
    "                print(\"Found too many names! Splitting along longest sides\")\n",
    "                try:\n",
    "                    shapely_contour = contours_to_shapely_polygons(contour)\n",
    "                    outpoly_1, outpoly_2 = splitPolygonByLongerSides(shapely_contour)\n",
    "                    \n",
    "                    outdict[text[0]] = convertShapelyToCV2(outpoly_1)\n",
    "                    outdict[text[1]] = convertShapelyToCV2(outpoly_2)\n",
    "                except:\n",
    "                    print(\"Failure! Results will be inaccurate due to line segment on Tile Boundary not being identified\")\n",
    "                    continue\n",
    "                continue\n",
    "            outdict[text] = contour\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    return outdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b63c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoundaryPoints(row):\n",
    "    # GET WHICHEVER POLYGON IS THE LARGEST IN THE ROW'S GEOMETRY AND SIMPLIFY\n",
    "    if row[\"geometry\"] is None:\n",
    "        return None\n",
    "    elif isinstance(row[\"geometry\"], MultiPolygon):\n",
    "        largest_polygon_area = np.argmax([a.area for a in row[\"geometry\"]])\n",
    "        largest_polygon = row[\"geometry\"][largest_polygon_area].simplify(tolerance=20)\n",
    "        largest_polygon = largest_polygon.boundary\n",
    "    else:\n",
    "        largest_polygon = row[\"geometry\"].boundary\n",
    "    \n",
    "    # CONVERT POLYGON TO POINTS\n",
    "    length = largest_polygon.length # POLYGON LENGTH\n",
    "    point_boundary_list = list()    # OUTPUT POINT LIST\n",
    "    \n",
    "    # INTERPOLATE THROUGH ALL OF LENGTH\n",
    "    for distance in tqdm(range(0,int(length),20), disable=True):         \n",
    "        point = largest_polygon.interpolate(distance)   \n",
    "        point_boundary_list.append(point)\n",
    "    point_boundary_gdf = gpd.GeoDataFrame(geometry=point_boundary_list)\n",
    "    \n",
    "    # SHAPEFILE BOUNDS\n",
    "    shp_bounds = [i for i in largest_polygon.bounds]\n",
    "    return point_boundary_gdf, shp_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebbac5ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhace\\OneDrive - University Of Houston\\AAA_RECTDNN\\processing\\2023-09-08_11-24-10\\Inputs\\480233IND0_0382.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ccd0ef9069491da717a13c1af4285d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1656\\3385112914.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     effectiveArea, RLNN    = findSquares(image, model=RLNN,\n\u001b[0;32m     36\u001b[0m                                         \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                                         model_checkpoint=f\"{data_dir}RLNN/checkpoint_091323.pth\")\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tempfiles/{filename}_00_classification.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobability_to_rgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifications\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tempfiles/{filename}_00_effectiveArea.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobability_to_rgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meffectiveArea\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\FIRMsDigitizing\\RECTDNN\\FindGrid.py\u001b[0m in \u001b[0;36mfindSquares\u001b[1;34m(image, model, model_checkpoint, cnn_creation_params, device)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;31m# PROCESS IMAGE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_prep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\FIRMsDigitizing\\RECTDNN\\RLNN.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, resize)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mx\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0menc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\FIRMsDigitizing\\RECTDNN\\RLNN.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Apply convolutional layer and sigmoid activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# Multiply the input feature map by the attention map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 460\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "\n",
    "TPNN = None\n",
    "RLNN = None\n",
    "\n",
    "for i, row in index_files.iterrows():\n",
    "    print(row[\"FilePath\"])\n",
    "    \n",
    "    filename = os.path.basename(row[\"FilePath\"])\n",
    "    \n",
    "    # READ FILES AND RUN CLASSIFICATIONS\n",
    "    image = cv2.imread(row[\"FilePath\"])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # SAVE IMAGE TO OUTPUT DIRECTORY\n",
    "    output_image = os.path.join(outputs_dir, filename)\n",
    "    copyfile(row[\"FilePath\"], output_image)\n",
    "    \n",
    "    # GET BOUNDARY POINTS FROM RESPECTIVE SHAPEFILE\n",
    "    point_boundary_gdf, shp_bounds = getBoundaryPoints(row)\n",
    "    if point_boundary_gdf is None:\n",
    "        continue\n",
    "    \n",
    "    # WHAT ARE WE LOOKING FOR IN EACH IDENTIFIED SQUARE?\n",
    "    key = findKey(row[\"Basename\"])    \n",
    "    if key is None:\n",
    "        print(f\"Could not find key in {filename}\")\n",
    "        \n",
    "    data_dir = r\"C:\\\\Users\\\\fhace\\\\OneDrive - University Of Houston\\\\AAA_RECTDNN\\\\data\\\\\"\n",
    "        \n",
    "    # RUN IMAGES THROUGH CNNs\n",
    "    classifications, TPNN  = findKeypoints(image, model=TPNN, \n",
    "                                          device=\"cpu\", \n",
    "                                          model_checkpoint=f\"{data_dir}TPNN/checkpoint_091523_pyramids_2.pth\")\n",
    "    effectiveArea, RLNN    = findSquares(image, model=RLNN,\n",
    "                                        device=\"cpu\", \n",
    "                                        model_checkpoint=f\"{data_dir}RLNN/checkpoint_091323.pth\")\n",
    "    cv2.imwrite(f\"tempfiles/{filename}_00_classification.png\", np.asarray(probability_to_rgb(classifications)))\n",
    "    cv2.imwrite(f\"tempfiles/{filename}_00_effectiveArea.png\", np.asarray(probability_to_rgb(effectiveArea)))\n",
    "    error\n",
    "    # FIND THE GRID INSIDE THE IMAGE\n",
    "    out = FindGrid(image, classifications, effectiveArea, key, row[\"FilePath\"], verbose=True)\n",
    "    \n",
    "    # PLOT IMAGE\n",
    "    plot_image = np.dstack([image, image, image])\n",
    "    contours = []\n",
    "    \n",
    "    \n",
    "    # LOOP THROUGH CONTOURS AND ADD THEM TO IMAGE\n",
    "    for k, v in out.items():\n",
    "        cv2.drawContours(plot_image, v, -1, (0, 255, 0), 3)\n",
    "        contours.append(contours_to_shapely_polygons(v))\n",
    "        \n",
    "    # CALCULATE BOUNDS OF CONTOURS ON IMAGE\n",
    "    bounds_panels = MultiPolygon(contours).bounds\n",
    "    bounds_panels = [int(i) for i in bounds_panels]\n",
    "\n",
    "    # CREATE MASK USING GRID \n",
    "    mask = np.zeros(image.shape)\n",
    "    mask[bounds_panels[1]+50:bounds_panels[3]-50, bounds_panels[0]+50:bounds_panels[2]-50] = 1\n",
    "    mask_1 = drawGrid(image, out)          # GRID MASKING\n",
    "    mask = np.logical_and(~mask_1, mask)   # MERGE MASKS\n",
    "    \n",
    "    # FLIP IMAGE AND MASK IT\n",
    "    image = 255 - image\n",
    "    image_t = image * mask\n",
    "    \n",
    "    # RASTER BOUNDS\n",
    "    get_bounds_image = np.where(np.asarray(cv2.erode(image_t, np.ones((3,3), np.uint8)) > 50), 1, 0)\n",
    "    # plt.imshow(get_bounds_image)\n",
    "    # plt.show()\n",
    "    \n",
    "    # print(get_bounds_image)\n",
    "    bounds_panels_postfilter = find_bbox(get_bounds_image)\n",
    "    bounds_panels = [i for i in bounds_panels_postfilter]\n",
    "\n",
    "    print(\"Assigning Points\")\n",
    "    # INITIAL TRANSFORM\n",
    "    from_points   = np.array([[bounds_panels[0], bounds_panels[1]], \n",
    "                   [bounds_panels[0], bounds_panels[3]],\n",
    "                   [bounds_panels[2], bounds_panels[1]],\n",
    "                   [bounds_panels[2], bounds_panels[3]]])\n",
    "\n",
    "    to_points = np.array([[shp_bounds[0], shp_bounds[1]], \n",
    "                   [shp_bounds[0], shp_bounds[3]],\n",
    "                   [shp_bounds[2], shp_bounds[1]],\n",
    "                   [shp_bounds[2], shp_bounds[3]]])\n",
    "\n",
    "    initial_transform = cv2.findHomography(from_points, to_points, cv2.RANSAC, 1000)\n",
    "    original_homography = initial_transform[0]\n",
    "    inverse_transform = np.linalg.inv(original_homography)\n",
    "\n",
    "    # CONVERT THINNED IMAGE TO POINTS\n",
    "    thin_image = cv2.ximgproc.thinning(image_t.astype(np.uint8), thinningType=cv2.ximgproc.THINNING_GUOHALL)\n",
    "    y, x = np.where(thin_image[::-1, :])                   # GET COORDINATES OF EVERY \n",
    "    image_points = np.vstack((x, y, np.ones(x.shape)))     # STACK X, Y, AND Z COORDINATES\n",
    "    # outputs = original_homography @ image_points           # TRANSFORM COORDINATES USING ESTIMATES\n",
    "    \n",
    "    # TRANSFORM SHAPEFILE POINTS INTO IMAGE COORDINATE SYSTEM\n",
    "    point_geometry = [[point.geometry.x, point.geometry.y, 1] for i, point in point_boundary_gdf.iterrows()]\n",
    "    point_geometry = inverse_transform @ np.array(point_geometry).T\n",
    "    \n",
    "    # COORDINATE HANDLING\n",
    "    coords_shp = point_geometry.T\n",
    "    coords_ras = np.vstack((image_points[0, :], image_points[1, :], np.ones(image_points[1, :].shape))).T\n",
    "    \n",
    "    # IMAGE ORIGIN COORDINATE SYSTEM TO IMAGE CENTER COORDINATE SYSTEM\n",
    "    offsets = np.mean(coords_ras, axis=0)\n",
    "    x_offset = offsets[0]\n",
    "    y_offset = offsets[1]\n",
    "\n",
    "    coords_shp_proc = np.vstack((coords_shp[:, 0] - x_offset, coords_shp[:, 1] - y_offset)).T\n",
    "    coords_ras_proc = np.vstack((coords_ras[:, 0] - x_offset, coords_ras[:, 1] - y_offset)).T\n",
    "    \n",
    "    # FAST SEARCH STRUCTURE\n",
    "    kdtree = cKDTree(coords_ras_proc)\n",
    "    \n",
    "    # ITERATIVE CLOSEST POINT\n",
    "    reprojected_points = []\n",
    "    compounded_homography = np.eye(3)\n",
    "    proc_points = coords_shp_proc\n",
    "    \n",
    "    # ITERATE\n",
    "    for i in tqdm(range(20), disable=True):\n",
    "        # TAKE ADJUSTMENT STEP\n",
    "        out, new_homography = adjustStep(proc_points, coords_ras_proc, kdtree,\n",
    "                                        shear=False, rotation=False, perspective=False)\n",
    "        compounded_homography = compounded_homography @ new_homography\n",
    "        reprojected_points.append(out)\n",
    "        proc_points = out\n",
    "        if i % 10 == 0:\n",
    "            scale  = np.sqrt((new_homography[0, 0] ** 2 + new_homography[1, 1] ** 2) / 2)\n",
    "            offset = np.sqrt((new_homography[1, 2] ** 2 + new_homography[0, 2] ** 2) / 2)\n",
    "\n",
    "            print(f\"Scale: {scale:.2f} Offset: {offset:.2f}\")\n",
    "    \n",
    "    plotICP(reprojected_points, plot_skip=10, )\n",
    "    plt.show()\n",
    "    \n",
    "    # REVERSE Y AXIS\n",
    "    rev_y_axis = np.array([[1, 0, 0],\n",
    "                           [0,-1, 0],\n",
    "                           [0, 0, 1]])\n",
    "\n",
    "    # move = original_homography @ np.array([0, image_t.shape[0], 0])\n",
    "    translation = np.eye(3)\n",
    "    translation[1, 2] = image_t.shape[0]\n",
    "\n",
    "    adjustment = np.linalg.inv(compounded_homography.copy())\n",
    "    adjustment[0, 2] = -1 * adjustment[0, 2]\n",
    "    adjustment[1, 2] = -1 * adjustment[1, 2]\n",
    "    # temp = adjustment[0, 2]\n",
    "    # adjustment[0, 2] = adjustment[1, 2]\n",
    "    # adjustment[1, 2] = temp\n",
    "    output_transform = original_homography @ adjustment @ translation @ rev_y_axis \n",
    "    print(output_transform)\n",
    "    # output_transform = original_homography @ translation @ rev_y_axis \n",
    "      \n",
    "    output_affine = Affine(*output_transform.flatten()[:6])\n",
    "    write_world_file_from_affine(output_affine, get_world_file_path(output_image))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
