{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4333b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON IMPORTS\n",
    "import os\n",
    "import copy\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# IMAGE IMPORTS \n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# DATA IMPORTS \n",
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# NEURAL NETWORK\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import ToPILImage, GaussianBlur\n",
    "from torchvision.transforms import Compose, RandomCrop, ToTensor, Normalize\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision.models as models\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# MY OWN CLASSES\n",
    "from SquareLocator import *\n",
    "Image.MAX_IMAGE_PIXELS = 933120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874397a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r\"C:\\Users\\fhacesga\\OneDrive - University Of Houston\\AAA_RECTDNN\\data\"\n",
    "\n",
    "input_folder = f\"{base_dir}/SquareLocator/in\"\n",
    "val_folder = f\"{base_dir}/SquareLocator/in\"\n",
    "target_folder = f\"{base_dir}/SquareLocator/out\"\n",
    "batch_size = 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=180),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = SquareDataset_Multiclass(input_folder, target_folder, transform=transform, \n",
    "                                         crop=False, \n",
    "                                         resize=True,\n",
    "                                         resize_def=64,\n",
    "                                        )\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "loaders = {'train' : train_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97acd5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloaders, num_epochs=50, \n",
    "          output_dir=f'{base_dir}/SquareLocator/intermediate_outputs', \n",
    "          learning_rate=5e-4):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    weights = torch.tensor([1, 120]).float().to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights, \n",
    "        reduction=\"mean\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    learning_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.97)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = model.to(device)\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train']: \n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                repeats = range(4)\n",
    "            else:\n",
    "                model.eval()\n",
    "                repeats = range(2)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            outputs_folder = os.path.join(output_dir, phase)\n",
    "            if not os.path.exists(outputs_folder):\n",
    "                os.makedirs(outputs_folder)\n",
    "                \n",
    "            curr_loss = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for rep_id in tqdm(repeats):\n",
    "                if rep_id % 2 == 0 and rep_id != 0:\n",
    "                    print(f\"{curr_loss:.4e} {curr_loss/8:.4e}\")\n",
    "                    curr_loss = 0\n",
    "                for inputs, labels, filenames in dataloaders[phase]:\n",
    "                    \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels) \n",
    "                    curr_loss += loss\n",
    "                    if rep_id % 2 == 0 and phase is 'train' and rep_id != 0:\n",
    "                        prob_img_or = outputs.detach().cpu()\n",
    "                        minputs = inputs.detach().cpu().numpy()\n",
    "                        \n",
    "                        if prob_img_or.ndim == 3:\n",
    "                            prob_img_or = prob_img_or.unsqueeze(0)\n",
    "                            \n",
    "                        prob_img_or = prob_img_or.numpy()\n",
    "                        \n",
    "                        for i in range(len(outputs)):\n",
    "                            filename = filenames[i]\n",
    "                            for ii in range(prob_img_or.shape[1]):\n",
    "                                prob_img = prob_img_or[i, ii, :, :]\n",
    "                                prob_img = (prob_img * 255).astype(np.uint8)\n",
    "                                prob_img = Image.fromarray(np.squeeze(prob_img))\n",
    "                                prob_img.save(os.path.join(outputs_folder, f\"{rep_id}_{ii}_{filename}\"))\n",
    "                            myinp = Image.fromarray(np.uint8(minputs[i, 0, :, :] * 255))\n",
    "                            myinp.save(os.path.join(outputs_folder, f\"{rep_id}_{filename[:-3]}_inp.png\"))\n",
    "                    if phase is 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_acc:\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        # Update the learning rate scheduler after each epoch\n",
    "        learning_rate_scheduler.step()\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        # Save the model and optimizer states\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, f'{base_dir}/SquareLocator/checkpoint_072623.pth')\n",
    "            \n",
    "            torch.save(model, f\"{base_dir}/SquareLocator/072623.pth\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8a83ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhacesga\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "C:\\Users\\fhacesga\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ab6b3731f844afaede3a9823b7ced4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n",
      "torch.float32\n",
      "torch.Size([1, 64, 64])\n",
      "torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\RECTDNN\\SquareLocator.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 64, 64])\n",
      "torch.float32\n",
      "torch.Size([1, 1, 64, 64])\n",
      "torch.float32\n",
      "torch.Size([1, 64, 64])\n",
      "torch.int64\n",
      "torch.Size([1, 2, 64, 64])\n",
      "torch.float32\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 8.00 GiB total capacity; 6.35 GiB already allocated; 0 bytes free; 7.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16332\\2773947076.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSquareLocator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16332\\2818951294.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloaders, num_epochs, output_dir, learning_rate)\u001b[0m\n\u001b[0;32m     80\u001b[0m                             \u001b[0mmyinp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{rep_id}_{filename[:-3]}_inp.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m                         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 8.00 GiB total capacity; 6.35 GiB already allocated; 0 bytes free; 7.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = SquareLocator()\n",
    "model = train(model, loaders, num_epochs=500, learning_rate=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
