{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b80c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON IMPORTS\n",
    "import os\n",
    "import copy\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# IMAGE IMPORTS \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import tifffile\n",
    "\n",
    "# DATA IMPORTS \n",
    "import random\n",
    "import h5py\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# NEURAL NETWORK\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import ToPILImage, GaussianBlur\n",
    "from torchvision.transforms import Compose, RandomCrop, ToTensor, Normalize\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision.models as models\n",
    "\n",
    "# MY OWN CLASSES\n",
    "from TileLocator import *\n",
    "\n",
    "# PREFERENCES\n",
    "Image.MAX_IMAGE_PIXELS = 933120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "face9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_dir = \"data/templates/\"\n",
    "tempfiles_dir = \"tempfiles/\"\n",
    "\n",
    "boundary_shapefile = f\"{templates_dir}HCAD_Harris_County_Boundary.shp\"\n",
    "boundary_points    = f'{tempfiles_dir}boundary_points.shp'\n",
    "roads_points       = f'{tempfiles_dir}roads_points.shp'\n",
    "\n",
    "tile_file = f\"data/TileIndices/48201CIND0_0992.tif\"\n",
    "\n",
    "roads_fn = f\"data/Roads/TexasHighways.shp\"\n",
    "model_checkpoint = \"data/TileLocator/checkpoint_071723.pth\"\n",
    "\n",
    "class_names = [\"Tiles\", \"Roads\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1052056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_run_cnn(image_path, model, tilesize=1024, overhang_size=2):\n",
    "        \n",
    "    tensor = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    num_classes = 2\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Calculate the number of tiles needed\n",
    "    width, height = image.size\n",
    "    num_tiles_x = (width + tilesize-1) // tilesize\n",
    "    num_tiles_y = (height + tilesize-1) // tilesize\n",
    "    \n",
    "    # Create an empty list to store the output tiles\n",
    "    output_tiles = []\n",
    "    \n",
    "    output_gen = np.zeros((width, height, num_classes))\n",
    "    \n",
    "    # Iterate over each tile\n",
    "    for tile_x in tqdm(range(num_tiles_x)):\n",
    "        for tile_y in range(num_tiles_y):\n",
    "                        \n",
    "            # Calculate the coordinates for the current tile\n",
    "            x0 = tile_x * tilesize\n",
    "            y0 = tile_y * tilesize\n",
    "            x1 = min(x0 + tilesize, width)\n",
    "            y1 = min(y0 + tilesize, height)\n",
    "            \n",
    "            # Crop the image to the current tile\n",
    "            tile = image.crop((x0, y0, x1, y1))\n",
    "            \n",
    "            # Pad the tile if needed\n",
    "            pad_width = tilesize - tile.width\n",
    "            pad_height = tilesize - tile.height\n",
    "            if pad_width > 0 or pad_height > 0:\n",
    "                padding = ((0, pad_height), (0, pad_width))\n",
    "                tile = np.pad(tile, padding, mode='constant')\n",
    "            \n",
    "            # Preprocess the tile\n",
    "            tile = np.array(tile)\n",
    "            \n",
    "            #if np.max(tile) == 1:\n",
    "            #    tile = tile * 255\n",
    "            \n",
    "            # tile = np.where(tile > 127, 255, 0).astype(np.uint8)\n",
    "            \n",
    "            tile_tensor = tensor(tile).unsqueeze(0).to(\"cuda\")\n",
    "            \n",
    "            # Run the CNN on the tile\n",
    "            output = model(tile_tensor)\n",
    "            output = output[0, 1:, :, :].cpu().detach().numpy().T\n",
    "            \n",
    "            # Store the output tile\n",
    "            \n",
    "            x_fin = tilesize - pad_width\n",
    "            y_fin = tilesize - pad_height\n",
    "            \n",
    "            temp = output[0:x_fin, 0:y_fin, :]\n",
    "            \n",
    "            temp[:, :overhang_size, :] = 0\n",
    "            temp[:, overhang_size:, :] = 0\n",
    "            temp[:, :, overhang_size:] = 0\n",
    "            temp[:, :, :overhang_size] = 0\n",
    "            \n",
    "            output_gen[x0:x1, y0:y1, :] = temp\n",
    "        torch.cuda.empty_cache()\n",
    "    return output_gen.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e51deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "# model = torch.load(model_checkpoint)# RectangleClass()\n",
    "# model = model.to(\"cuda\")\n",
    "model = RectangleClass(num_classes=3)\n",
    "checkpoint = torch.load(model_checkpoint)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ea11617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb3a9a1cd8e459e933c74e1447a737b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_folder = f\"data/TileIndicesStore/\"\n",
    "output_folder = f\"data/TileIndices_Sharpened/\"\n",
    "    \n",
    "def makeKernel(w):\n",
    "    kernel = np.ones((w, w)) * 0\n",
    "    kernel[w // 2, :] = -1\n",
    "    kernel[:, w // 2] = -1\n",
    "    kernel[w // 2, w // 2] = kernel.size + np.sum(kernel) + 1\n",
    "    return kernel\n",
    "    \n",
    "if True:\n",
    "     # Iterate over the files in the input folder\n",
    "    for filename in tqdm(os.listdir(input_folder)):\n",
    "        # Check if the file has a supported image extension\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "            # Open the image file\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename[:-3] + \"png\")\n",
    "\n",
    "            image = cv2.imread(image_path, 0)\n",
    "            \n",
    "            # Get the original image dimensions\n",
    "            height, width = image.shape[:2]\n",
    "            max_size = 14440\n",
    "\n",
    "            # Determine the scaling factor\n",
    "            scale = max_size / max(height, width)\n",
    "\n",
    "            # Calculate the new dimensions\n",
    "            new_height = int(height * scale)\n",
    "            new_width = int(width * scale)\n",
    "\n",
    "            # Resize the image using the calculated dimensions\n",
    "            image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "\n",
    "            # Create a kernel\n",
    "            kernel = makeKernel(4)\n",
    "\n",
    "            # Apply the kernel to the image\n",
    "            processed_image = cv2.filter2D(image, -1, kernel).astype(np.uint8)\n",
    "            \n",
    "            # processed_image = image\n",
    "            \n",
    "            # processed_image = cv2.adaptiveThreshold(processed_image, 255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            #                        cv2.THRESH_BINARY,19, 8)\n",
    "\n",
    "            # Save the processed image to the output folder\n",
    "            cv2.imwrite(output_path, processed_image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b7192",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 480233IND0_0382.png and saved to data/TileIndicesBoundaries/480233IND0_0382.tif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1914214aa46046a991bf45c0fbc07806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "D:\\Desktop\\FIRMsDigitizing\\RECTDNN\\TileLocator.py:93: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 480287IND0_0281.png and saved to data/TileIndicesBoundaries/480287IND0_0281.tif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e203604a30a4f13800b7d64daf942ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 480287IND0_0288.png and saved to data/TileIndicesBoundaries/480287IND0_0288.tif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3f52e6d37e42da82bb85879adebacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 480287IND0_0382.png and saved to data/TileIndicesBoundaries/480287IND0_0382.tif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97596cf384f6431f8efbf8b45bd9143e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 480287IND0_0985.png and saved to data/TileIndicesBoundaries/480287IND0_0985.tif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb02d3c09c64884a92ced3c919a3610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 480296IND0_0982.png and saved to data/TileIndicesBoundaries/480296IND0_0982.tif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af905a0e774c4ffa9fd9787a29d79c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_folder = f\"data/TileIndices_Sharpened/\"\n",
    "output_folder = f\"data/TileIndicesBoundaries/\"\n",
    "    \n",
    "# Iterate over the files in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    # Check if the file has a supported image extension\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "        # Open the image file\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename[:-3] + \"tif\")\n",
    "        print(f\"Processing {filename} and saved to {output_path}\")\n",
    "        \n",
    "        # Apply the process function to the image\n",
    "        processed_image = split_and_run_cnn(image_path, model)\n",
    "        \n",
    "        processed_image = np.dstack((\n",
    "            processed_image[0, :, :],\n",
    "            processed_image[1, :, :],\n",
    "            np.zeros(processed_image[0, :, :].shape)\n",
    "        ))\n",
    "        \n",
    "        image = processed_image * 255 \n",
    "        \n",
    "        cv2.imwrite(output_path, image.astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
