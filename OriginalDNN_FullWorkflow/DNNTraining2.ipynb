{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e89c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from osgeo import gdal, osr\n",
    "import pyproj\n",
    "from shutil import copyfile\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a58bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeorectDataset(Dataset):\n",
    "    def __init__(self, base_raster, image_paths,size_x = 3000, size_y=2000, scaled=True):\n",
    "                \n",
    "        self.toTensor = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.Resize((size_x, size_y))\n",
    "        ])\n",
    "        \n",
    "        self.base_raster = base_raster\n",
    "        self.base = Image.open(self.base_raster)\n",
    "        shape = np.asarray(self.base).shape\n",
    "        self.base = self.toTensor(self.base)[0, :, :].float().unsqueeze(0)\n",
    "        \n",
    "        scaling = np.array([\n",
    "                [size_x / shape[0],                   0],\n",
    "                [                0,   size_y / shape[1]]])\n",
    "        \n",
    "        self.image_paths = image_paths\n",
    "        raw_affine = list()\n",
    "        scaled_affine = list()\n",
    "        scales = list()\n",
    "        \n",
    "        for path in tqdm(self.image_paths):\n",
    "            affine = np.load(path[:-4]+\"_affine.npy\").reshape(2, 3)\n",
    "            \n",
    "            sc_affine = scaling @ affine\n",
    "            \n",
    "            raw_affine.append(affine.flatten())\n",
    "            scaled_affine.append(sc_affine.flatten())\n",
    "            scales.append(scaling.flatten())\n",
    "        \n",
    "        if scaled:\n",
    "            self.affine = np.vstack(scaled_affine)\n",
    "        else:\n",
    "            self.affine = np.vstack(raw_affine)\n",
    "            \n",
    "        self.raw_affine = raw_affine\n",
    "        self.scaled_affine = scaled_affine\n",
    "        self.scales = scales\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img1_path = self.image_paths[index]\n",
    "        img1 = self.toTensor(Image.open(img1_path))\n",
    "        img1 = torch.where(img1, 1, 255).float()\n",
    "        affine = self.affine[index, :]\n",
    "        return self.base, img1, affine\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f443784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        channels = x.shape[1]\n",
    "        y = self.avg_pool(x).view(batch_size, channels)\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = self.fc2(y).sigmoid()\n",
    "        if x.dim() == 4:\n",
    "            y = y.view(batch_size, channels, 1, 1)\n",
    "        elif x.dim() == 3:\n",
    "            y = y.view(batch_size, channels, 1)\n",
    "        \n",
    "        return x * y\n",
    "        \n",
    "        # Reshape to (batch_size, channels, 1, 1)\n",
    "        out = out.unsqueeze(2)\n",
    "        out = out.unsqueeze(3)\n",
    "        # Multiply by input tensor to compute output\n",
    "        out = x * out.expand_as(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e9ad646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention block module that takes in a 3D tensor and applies an attention mechanism to the channels dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # Define the attention mechanism layers\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compute the query, key, and value tensors\n",
    "        batch_size, _, width, height = x.size()\n",
    "        query = self.query_conv(x).view(batch_size, -1, width*height).permute(0, 2, 1)\n",
    "        key = self.key_conv(x).view(batch_size, -1, width*height)\n",
    "        value = self.value_conv(x).view(batch_size, -1, width*height)\n",
    "        \n",
    "        # Compute the attention map and attention output\n",
    "        attention = torch.bmm(query, key)\n",
    "        attention = self.softmax(attention)\n",
    "        attention_output = torch.bmm(value, attention.permute(0, 2, 1))\n",
    "        attention_output = attention_output.view(batch_size, self.in_channels, width, height)\n",
    "        \n",
    "        # Apply the attention output to the input tensor\n",
    "        out = self.gamma * attention_output + x\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5348af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeorectNet(nn.Module):\n",
    "    def __init__(self, num_scales = 3, size=(600, 400), channels=8, output_size=4):\n",
    "        super(GeorectNet, self).__init__()\n",
    "        \n",
    "        # PARAMS\n",
    "        self.size = size\n",
    "        self.num_scales = num_scales        \n",
    "        self.channels = channels\n",
    "        \n",
    "        # UPSAMPLING FOR LOWER-RES PYRAMID\n",
    "        self.upsample = nn.Upsample(size=size)\n",
    "        \n",
    "        # ATTENTION BLOCK\n",
    "        self.AttentionBlock = AttentionBlock(self.channels)\n",
    "        self.AttentionBlock_pyramid = AttentionBlock(self.channels * self.num_scales * 2)\n",
    "        \n",
    "        # SE BLOCK\n",
    "        self.SEBlock = SEBlock(self.channels)\n",
    "        self.SE_pyramid = SEBlock(self.channels * 2)\n",
    "        \n",
    "        # CONVOLUTIONAL LAYERS\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, self.channels // 2, kernel_size=5, stride=3, padding=11),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.channels // 2, self.channels, kernel_size=5, stride=3, padding=11),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "            \n",
    "        fin_conv = nn.Conv2d(\n",
    "            in_channels=self.channels, \n",
    "            out_channels=output_size, \n",
    "            kernel_size=size\n",
    "        )\n",
    "        \n",
    "        class Flatten(nn.Module):\n",
    "            def forward(self, x):\n",
    "                return x.view(x.size(0), -1)\n",
    "        \n",
    "        self.fin_conv = nn.Sequential(\n",
    "            fin_conv,\n",
    "            Flatten()\n",
    "        )\n",
    "\n",
    "    def pyramidStep(self, x1, x2):\n",
    "        x1 = self.AttentionBlock(self.SEBlock(self.conv(x1)))\n",
    "        x2 = self.AttentionBlock(self.SEBlock(self.conv(x2)))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        outputs = list()\n",
    "        \n",
    "        # FOR EACH SCALE IN PYRAMID\n",
    "        for i in range(self.num_scales):\n",
    "            \n",
    "            # DOWNSAMPLE\n",
    "            scale = 1.0 / 2**i\n",
    "            size = (int(x1.size()[-2] * scale), int(x1.size()[-1] * scale))\n",
    "            x1_scale = F.interpolate(x1, size=size, mode='bilinear')\n",
    "            x2_scale = F.interpolate(x2, size=size, mode='bilinear')\n",
    "            \n",
    "            # RUN SINGLE STEP \n",
    "            output_transformation = self.pyramidStep(x1_scale, x2_scale)\n",
    "            \n",
    "            # UPSAMPLE AND APPEND\n",
    "            output_transformation = self.upsample(output_transformation)\n",
    "            outputs.append(output_transformation)\n",
    "            \n",
    "        # OUTPUT ATTENTION AND SE BLOCKS\n",
    "        outputs = torch.cat(outputs, 1)\n",
    "        print(outputs.shape)\n",
    "        outputs = self.AttentionBlock_pyramid(outputs)\n",
    "        outputs = self.SE_pyramid(outputs)\n",
    "        \n",
    "        # FINAL CONVOLUTION\n",
    "        current_affine = self.fin_conv(outputs)\n",
    "        \n",
    "        cuda0 = torch.device('cuda:0')\n",
    "        zeros = torch.zeros(current_affine.shape[0], device=cuda0)\n",
    "        \n",
    "        affine = torch.vstack((current_affine[:, 0], \n",
    "                               zeros, \n",
    "                               current_affine[:, 1], \n",
    "                               zeros, \n",
    "                               current_affine[:, 2], \n",
    "                               current_affine[:, 3])).T\n",
    "        \n",
    "        return affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe128da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_corners(image):\n",
    "    \"\"\"\n",
    "    Computes the corner coordinates of an image as a tensor.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor): A 2D or 3D tensor representing an image.\n",
    "    \n",
    "    Returns:\n",
    "        corners (torch.Tensor): A tensor of shape (4, 2) containing the corner coordinates of the image,\n",
    "            in the order (top-left, top-right, bottom-left, bottom-right).\n",
    "    \"\"\"\n",
    "    # Convert the input tensor to float and normalize it\n",
    "    image = image.float() / 255.0\n",
    "    \n",
    "    # Compute the corner coordinates of the image\n",
    "    height, width = image.shape[-2], image.shape[-1]\n",
    "    corners = torch.tensor([[0, 0, 1], [width, 0, 1], [0, height, 1], [width, height, 1]], dtype=torch.float32, device=image.device)\n",
    "    \n",
    "    return corners\n",
    "\n",
    "def corner_loss(src_affine_params, tgt_affine_params, image):\n",
    "    \"\"\"\n",
    "    Calculates the loss based on the difference between the corners of the source and target images,\n",
    "    after applying the affine transformations defined by the input parameters to the original image.\n",
    "    \n",
    "    Args:\n",
    "        src_affine_params (torch.Tensor): A 2D tensor of shape (batch_size, 6) containing the affine transformation\n",
    "            parameters for the source image, in the order (a11, a12, tx, a21, a22, ty).\n",
    "        tgt_affine_params (torch.Tensor): A 2D tensor of shape (batch_size, 6) containing the affine transformation\n",
    "            parameters for the target image, in the same order as src_affine_params.\n",
    "        image (torch.Tensor): A 4D tensor of shape (batch_size, channels, height, width) containing the original\n",
    "            input images.\n",
    "    \n",
    "    Returns:\n",
    "        corner_loss (torch.Tensor): A scalar tensor representing the loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    grid = get_image_corners(image).T\n",
    "    \n",
    "    corner_loss = 0\n",
    "    \n",
    "    for i in range(image.shape[0]):\n",
    "        \n",
    "        \n",
    "        src_affine_matrix = torch.vstack((torch.squeeze(src_affine_params[i]).view(2, 3), \n",
    "                                          torch.cuda.FloatTensor([0,0,1])))\n",
    "        tgt_affine_matrix = torch.vstack((torch.squeeze(tgt_affine_params[i]).view(2, 3), \n",
    "                                          torch.cuda.FloatTensor([0,0,1])))\n",
    "\n",
    "        src_corners = torch.matmul(src_affine_matrix, grid)\n",
    "        tgt_corners = torch.matmul(tgt_affine_matrix, grid)\n",
    "        curr_loss =  F.l1_loss(src_corners, tgt_corners)\n",
    "        corner_loss = corner_loss + curr_loss\n",
    "        \n",
    "    corner_loss = corner_loss / image.shape[0]\n",
    "    \n",
    "    return corner_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf0b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savetextfile(epoch, srcaffine, tgtaffine, intermediate_dir ):\n",
    "    tempt = np.dstack((srcaffine.detach().cpu(), tgtaffine.detach().cpu()))\n",
    "    \n",
    "    with open(f\"{intermediate_dir}/{epoch}.txt\", 'w') as f:\n",
    "       for row in tempt:\n",
    "           np.savetxt(f, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2896954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Georect_net(net,\n",
    "                      Georect_dataset, \n",
    "                      batch_size=1, \n",
    "                      num_epochs=300, \n",
    "                      learning_rate=0.0001, \n",
    "                      validation_split=0.2, \n",
    "                      device='cuda'):\n",
    "    \n",
    "    intermediate_dir = r\"C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\RECTDNN\\intermediate_outputs\"\n",
    "    \n",
    "    if not os.path.isdir(intermediate_dir):\n",
    "        os.makedirs(intermediate_dir)\n",
    "    \n",
    "    # Split dataset into training and validation sets\n",
    "    dataset_size = len(Georect_dataset)\n",
    "    val_size = int(dataset_size * validation_split)\n",
    "    train_size = dataset_size - val_size\n",
    "    train_dataset, val_dataset = random_split(Georect_dataset, [train_size, val_size])\n",
    "\n",
    "    # Create data loaders for training and validation sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    bestval = torch.tensor(float('inf'))\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the network\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (img1, img2, label) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = net(img1, img2)\n",
    "            \n",
    "            loss = criterion(output.float(), label.squeeze().float())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Evaluate the network on the validation set\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (img1, img2, label) in enumerate(val_loader):\n",
    "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "                output = net(img1, img2)\n",
    "                savetextfile(epoch, label, output, intermediate_dir )\n",
    "                corloss = corner_loss(output.float(), label.float(), img2)\n",
    "                loss = criterion(output.float(), label.squeeze().float())\n",
    "                val_loss += corloss\n",
    "                \n",
    "            if bestval > val_loss:\n",
    "                bestval = val_loss\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch, \n",
    "                    'model_state_dict': net.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict()\n",
    "                }\n",
    "                torch.save(checkpoint, \"checkpoint.pth\")\n",
    "\n",
    "        # Print training and validation loss for the epoch\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Corners: {:.4f}'.format(epoch+1, num_epochs, train_loss/len(train_loader), val_loss/len(val_loader), corloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aef849a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8cbce241d9421db3d012e3e3d1ffb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainloc = r\"C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\RECTDNN\\TrainDataset\\\\\"\n",
    "base_loc = r\"D:\\FloodChange\\BaseRaster\\BaseTest.tif\"\n",
    "\n",
    "files = glob.glob(f\"{trainloc}*.tif\")\n",
    "train_dataset = GeorectDataset(base_loc, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a96446d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhacesga\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a36e7ac4dad4bff87e5b5444e30bd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhacesga\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114: UserWarning: An output with one or more elements was resized since it had shape [1, 8], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 600, 400])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 214.58 GiB (GPU 0; 8.00 GiB total capacity; 506.41 MiB already allocated; 6.58 GiB free; 552.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11140\\478219941.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Define Georect network and move it to device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGeorectNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_Georect_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11140\\3686928145.py\u001b[0m in \u001b[0;36mtrain_Georect_net\u001b[1;34m(net, Georect_dataset, batch_size, num_epochs, learning_rate, validation_split, device)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11140\\3056413697.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttentionBlock_pyramid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSE_pyramid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11140\\2911192951.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Compute the attention map and attention output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 214.58 GiB (GPU 0; 8.00 GiB total capacity; 506.41 MiB already allocated; 6.58 GiB free; 552.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Define Georect network and move it to device\n",
    "net = GeorectNet().to('cuda')\n",
    "train_Georect_net(net, train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
