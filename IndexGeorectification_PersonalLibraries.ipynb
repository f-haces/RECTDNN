{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7621809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK IMPORTS\n",
    "import os, glob, zipfile, warnings\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from shutil import copyfile, rmtree\n",
    "from datetime import datetime\n",
    "\n",
    "# IMAGE IMPORTS\n",
    "from PIL import Image\n",
    "\n",
    "# GIS IMPORTS\n",
    "from affine import Affine\n",
    "from shapely.geometry import  MultiPolygon\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# PLOTTING IMPORTS\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "# CUSTOM UTILITIES\n",
    "\n",
    "from IndexUtils import * \n",
    "from TPNN import *\n",
    "# from FindGrid import *\n",
    "\n",
    "# from WorldFileUtils import *\n",
    "# from GeometryUtils import *\n",
    "# from icp import *\n",
    "# from DataUtils import *\n",
    "# from FindGrid import *\n",
    "# from PlottingUtils import *\n",
    "# from affinetransformation import *\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = 933120000\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed09b63",
   "metadata": {},
   "source": [
    "IO directories depending on which machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34617015",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getlogin()\n",
    "\n",
    "if username == 'fhacesga':\n",
    "    base_input_path   = r\"D:\\FloodChange\\AAA_HistoricalDownload\"\n",
    "    base_output_path  = r\"C:\\Users\\\\\"+username+\"\\Desktop\\FIRMsDigitizing\\processing\"\n",
    "    ref_dir  = r\"C:\\Users\\fhacesga\\OneDrive - University Of Houston\\AAA_RECTDNN\\data\\AAA_ReferenceDatasets\\\\\"\n",
    "elif username == 'fhace':\n",
    "    ref_dir = r\"C:\\Users\\fhace\\OneDrive - University Of Houston\\AAA_RECTDNN\\data\\AAA_ReferenceDatasets\\\\\"\n",
    "else:\n",
    "    base_input_path   = r\"D:\\Desktop\\FIRMsDigitizing\\data\\HistoricalFIRMS\"\n",
    "    base_output_path  = r\"D:\\Desktop\\FIRMsDigitizing\\processing\"\n",
    "    ref_dir  = r\"C:\\Users\\franc\\OneDrive - University Of Houston\\AAA_RECTDNN\\data\\AAA_ReferenceDatasets\\\\\"\n",
    "\n",
    "init_databases(ref_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca1967",
   "metadata": {},
   "source": [
    "Create working dir and unzip all files if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca9abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_dir = None\n",
    "proc_dir = r\"C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-10-20_11-05-37\\\\\"\n",
    "\n",
    "if proc_dir is None:\n",
    "    datetime_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    proc_dir     = os.path.join(base_output_path, datetime_str)\n",
    "    unzipped_dir = os.path.join(proc_dir, \"Inputs\")\n",
    "    outputs_dir  = os.path.join(proc_dir, \"Outputs\")\n",
    "    print(proc_dir)\n",
    "    os.makedirs(proc_dir)\n",
    "    os.makedirs(unzipped_dir)\n",
    "    os.makedirs(outputs_dir)\n",
    "    extractZipFiles(base_input_path, unzipped_dir)\n",
    "else:\n",
    "    unzipped_dir = os.path.join(proc_dir, \"Inputs\")\n",
    "    outputs_dir  = os.path.join(proc_dir, \"Outputs\")\n",
    "    rmtree(outputs_dir)\n",
    "    os.makedirs(outputs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac9f90",
   "metadata": {},
   "source": [
    "Here we're using heuristics to identify indices within all the image files. These include:\n",
    "- Files that are shorter than 12 characters\n",
    "- Files that have the ```IND``` marker\n",
    "\n",
    "We create a Pandas DataFrame with the files matching. We then add several fields as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5180ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST ALL IMAGES IN DIRECTORY\n",
    "image_files = glob.glob(f\"{unzipped_dir}/*\")\n",
    "\n",
    "# FILTER IMAGES USING HEURISTICS\n",
    "patterns = [\"IND\", \"_1.\"]\n",
    "index_files = [file for pattern in patterns for file in glob.glob(unzipped_dir + \"\\\\*\" + pattern + \"*\")]\n",
    "filtered_files = [file for file in image_files if len(os.path.basename(file)) < 12]\n",
    "index_files.extend(filtered_files)\n",
    "\n",
    "# CREATE DATAFRAME\n",
    "index_files = pd.DataFrame(index_files, columns=[\"FilePath\"])\n",
    "\n",
    "# INDEX ATTRIBUTES TO BE ADDED\n",
    "index_files[\"Basename\"] = [os.path.basename(file) for file in index_files[\"FilePath\"].to_list()]    # BASENAME\n",
    "index_files[\"Location\"] = index_files[\"Basename\"].apply(extract_numerical_chars).astype(np.int32)   # \n",
    "index_files[\"GEOID\"]    = index_files[\"Location\"].apply(getGEOID)       # GET GEOID FOR EACH INDEX\n",
    "index_files[\"geometry\"] = index_files[\"GEOID\"].apply(getGeometry)       # GET GEOMETRY FROM MATCHING GEOIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544b54fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-10-20_11-05-37\\\\Inputs\\480233IND0_0382.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863487fc34c747d1af0c7741a622a786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[[    0.99983  5.8291e-08  1.7909e-05  8.2245e-05  7.0904e-05]\n",
      "  [    0.66145  8.7972e-05   0.0019368    0.058609     0.27792]\n",
      "  [    0.18043  0.00038815   0.0048263     0.18724     0.62711]\n",
      "  ...\n",
      "  [          1  3.3598e-30   1.438e-25  3.3718e-30  1.9013e-31]\n",
      "  [          1  4.8403e-29  4.6004e-24  6.2394e-28  2.1203e-29]\n",
      "  [          1  1.6087e-13   2.237e-11  3.5226e-12  2.8771e-10]]\n",
      "\n",
      " [[    0.77583  2.9799e-05  0.00043457    0.075057     0.14865]\n",
      "  [    0.12986  0.00016959  8.6374e-05    0.087065     0.78282]\n",
      "  [   0.028779   0.0005247  0.00023516     0.12728     0.84318]\n",
      "  ...\n",
      "  [          1  8.9097e-26   2.599e-23   6.167e-26  1.3662e-23]\n",
      "  [          1  2.2841e-21  1.3068e-19  1.9435e-20  2.1379e-17]\n",
      "  [    0.99987  9.6921e-10  2.5084e-11  2.1295e-08  0.00012694]]\n",
      "\n",
      " [[    0.47202  0.00017439   0.0011665    0.059813     0.46682]\n",
      "  [   0.023987  0.00022988  0.00066565    0.042667     0.93245]\n",
      "  [   0.027467  0.00023076    0.001937    0.010602     0.95976]\n",
      "  ...\n",
      "  [    0.98811  1.0786e-06  4.4275e-06  4.6115e-06    0.011883]\n",
      "  [    0.55601  1.3099e-05   8.536e-06  0.00026455      0.4437]\n",
      "  [   0.017787  5.0122e-05  2.0384e-05  0.00060835     0.98153]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  0.0015201  2.3906e-08  3.0106e-08  2.3431e-08     0.99848]\n",
      "  [   0.003503  4.7816e-08  1.5334e-08  7.5432e-09      0.9965]\n",
      "  [  0.0036148  3.4071e-06  3.0005e-08  3.8726e-07     0.99638]\n",
      "  ...\n",
      "  [          1           0           0           0           0]\n",
      "  [          1           0           0           0           0]\n",
      "  [          1           0           0           0           0]]\n",
      "\n",
      " [[    0.89689  4.7445e-07  2.4449e-07  7.1499e-07     0.10311]\n",
      "  [    0.91504  6.2827e-07  9.0358e-08  3.6844e-07    0.084962]\n",
      "  [    0.85145  2.7603e-05  6.5885e-07  3.1066e-06     0.14852]\n",
      "  ...\n",
      "  [          1  2.9708e-43     1.1e-42           0           0]\n",
      "  [          1  3.5282e-36  5.1811e-36  1.1003e-38  4.2039e-45]\n",
      "  [          1  1.0368e-22  1.7649e-25  1.0651e-24  1.3719e-28]]\n",
      "\n",
      " [[    0.99994  5.3887e-10  3.3624e-08  2.5657e-09  6.3627e-05]\n",
      "  [     0.9917  1.4137e-07  6.5239e-07  3.5489e-07   0.0083009]\n",
      "  [    0.95966  1.6649e-05  8.4396e-06  9.3999e-06    0.040303]\n",
      "  ...\n",
      "  [          1  2.9285e-24  1.0634e-22  2.3989e-26  2.9746e-30]\n",
      "  [          1  1.5449e-24  3.8825e-22  2.9447e-28  1.9512e-32]\n",
      "  [          1  3.0602e-16  7.5602e-16  2.0176e-20  1.8963e-22]]]\n",
      "\n",
      "image 1/1 C:\\Users\\fhacesga\\Desktop\\FIRMsDigitizing\\processing\\2023-10-20_11-05-37\\Inputs\\480233IND0_0382.jpg: 1024x1024 4 tiles, 1 county, 973.2ms\n",
      "Speed: 15.0ms preprocess, 973.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m countyArea, CLNN       \u001b[38;5;241m=\u001b[39m findCounty(image, model\u001b[38;5;241m=\u001b[39mCLNN)\n\u001b[0;32m     25\u001b[0m tiles, TLNN            \u001b[38;5;241m=\u001b[39m findTiles(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilePath\u001b[39m\u001b[38;5;124m\"\u001b[39m], model\u001b[38;5;241m=\u001b[39mTLNN)\n\u001b[1;32m---> 26\u001b[0m \u001b[43merror\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# GET BOUNDARY POINTS FROM RESPECTIVE SHAPEFILE\u001b[39;00m\n\u001b[0;32m     29\u001b[0m point_boundary_gdf, shp_bounds \u001b[38;5;241m=\u001b[39m getBoundaryPoints(row, distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "\n",
    "TPNN = None\n",
    "RLNN = None\n",
    "CLNN = None\n",
    "TLNN = None\n",
    "\n",
    "for i, row in index_files.iterrows():\n",
    "    print(row[\"FilePath\"])\n",
    "    \n",
    "    filename = os.path.basename(row[\"FilePath\"])\n",
    "    \n",
    "    # READ FILES AND CONVERT TO GRAYSCALE\n",
    "    image = cv2.imread(row[\"FilePath\"])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # SAVE IMAGE TO OUTPUT DIRECTORY\n",
    "    output_image = os.path.join(outputs_dir, filename)\n",
    "    copyfile(row[\"FilePath\"], output_image)\n",
    "    \n",
    "    # RUN IMAGES THROUGH DNNs\n",
    "    classifications, TPNN  = findKeypoints(image, model=TPNN)\n",
    "    effectiveArea, RLNN    = findSquares(image, model=RLNN)\n",
    "    countyArea, CLNN       = findCounty(image, model=CLNN)\n",
    "    tiles, TLNN            = findTiles(row[\"FilePath\"], model=TLNN)\n",
    "    error\n",
    "\n",
    "    # GET BOUNDARY POINTS FROM RESPECTIVE SHAPEFILE\n",
    "    point_boundary_gdf, shp_bounds = getBoundaryPoints(row, distance=100)\n",
    "    if point_boundary_gdf is None:\n",
    "        continue\n",
    "    \n",
    "    # WHAT ARE WE LOOKING FOR IN EACH IDENTIFIED SQUARE?\n",
    "    key = findKey(row[\"Basename\"])    \n",
    "    if key is None:\n",
    "        print(f\"Could not find key in {filename}\")\n",
    "        \n",
    "    # cv2.imwrite(f\"tempfiles/{filename}_00_classification.png\", np.asarray(probability_to_rgb(classifications)))\n",
    "    # cv2.imwrite(f\"tempfiles/{filename}_00_effectiveArea.png\", np.asarray(probability_to_rgb(effectiveArea)))\n",
    "    # cv2.imwrite(f\"tempfiles/{filename}_00_county.png\", np.asarray(probability_to_rgb(countyArea)))\n",
    "    \n",
    "    # FIND THE GRID INSIDE THE IMAGE\n",
    "    out = FindGrid(image, classifications, effectiveArea, key, row[\"FilePath\"], verbose=True)\n",
    "    \n",
    "    # PLOT IMAGE\n",
    "    plot_image = np.dstack([image, image, image])\n",
    "    contours = []\n",
    "    \n",
    "    # LOOP THROUGH CONTOURS AND ADD THEM TO IMAGE\n",
    "    for k, v in out.items():\n",
    "        cv2.drawContours(plot_image, v, -1, (0, 255, 0), 3)\n",
    "        contours.append(contours_to_shapely_polygons(v))\n",
    "        \n",
    "    # CALCULATE BOUNDS OF CONTOURS ON IMAGE\n",
    "    bounds_panels = MultiPolygon(contours).bounds\n",
    "    bounds_panels = [int(i) for i in bounds_panels]\n",
    "\n",
    "    # CREATE MASK USING GRID \n",
    "    mask = np.zeros(image.shape)\n",
    "    mask[bounds_panels[1]:bounds_panels[3], bounds_panels[0]:bounds_panels[2]] = 1\n",
    "    mask_1 = drawGrid(image, out)            # GRID MASKING\n",
    "    mask_2 = classifications[:, :, 1]        # CLASSIFICATIONS MAPPING\n",
    "    mask   = np.logical_and(~mask_1, mask)   # MERGE MASKS\n",
    "    mask   = np.logical_and(mask_2, mask)\n",
    "    \n",
    "    def translation_matrix(x, y, z=1):\n",
    "        matrix = np.array([[1, 0, x],\n",
    "                           [0, 1, y],\n",
    "                           [0, 0, z]])\n",
    "        return matrix\n",
    "        \n",
    "    \n",
    "    # FLIP IMAGE AND MASK IT\n",
    "    image = 255 - image\n",
    "    image_t = image * mask\n",
    "    \n",
    "    # RASTER BOUNDS\n",
    "    get_bounds_image = np.where(np.asarray(cv2.erode(image_t, np.ones((3,3), np.uint8)) > 50), 1, 0)\n",
    "    # plt.imshow(get_bounds_image)\n",
    "    # plt.show()\n",
    "    \n",
    "    # print(get_bounds_image)\n",
    "    bounds_panels_postfilter = find_bbox(get_bounds_image)\n",
    "    bounds_panels = [i for i in bounds_panels_postfilter]\n",
    "\n",
    "    print(\"Assigning Points\")\n",
    "    # INITIAL TRANSFORM\n",
    "    from_points   = np.array([[bounds_panels[0], bounds_panels[1]], \n",
    "                   [bounds_panels[0], bounds_panels[3]],\n",
    "                   [bounds_panels[2], bounds_panels[1]],\n",
    "                   [bounds_panels[2], bounds_panels[3]]])\n",
    "\n",
    "    to_points = np.array([[shp_bounds[0], shp_bounds[1]], \n",
    "                   [shp_bounds[0], shp_bounds[3]],\n",
    "                   [shp_bounds[2], shp_bounds[1]],\n",
    "                   [shp_bounds[2], shp_bounds[3]]])\n",
    "\n",
    "    initial_transform = cv2.findHomography(from_points, to_points, cv2.RANSAC, 1000)\n",
    "    original_homography = initial_transform[0]\n",
    "    inverse_transform = np.linalg.inv(original_homography)\n",
    "\n",
    "    # CONVERT THINNED IMAGE TO POINTS\n",
    "    thin_image = getCountyBoundaryFromImage(countyArea)\n",
    "    # cv2.ximgproc.thinning(image_t.astype(np.uint8), thinningType=cv2.ximgproc.THINNING_GUOHALL)\n",
    "    y, x = np.where(thin_image[::-1, :])                   # GET COORDINATES OF EVERY \n",
    "    image_points = np.vstack((x, y, np.ones(x.shape)))     # STACK X, Y, AND Z COORDINATES\n",
    "    # outputs = original_homography @ image_points           # TRANSFORM COORDINATES USING ESTIMATES\n",
    "    \n",
    "    # TRANSFORM SHAPEFILE POINTS INTO IMAGE COORDINATE SYSTEM\n",
    "    point_geometry = [[point.geometry.x, point.geometry.y, 1] for i, point in point_boundary_gdf.iterrows()]\n",
    "    point_geometry = inverse_transform @ np.array(point_geometry).T\n",
    "    \n",
    "    # COORDINATE HANDLING\n",
    "    coords_shp = point_geometry.T\n",
    "    coords_ras = np.vstack((image_points[0, :], image_points[1, :], np.ones(image_points[1, :].shape))).T\n",
    "    \n",
    "    # IMAGE ORIGIN COORDINATE SYSTEM TO IMAGE CENTER COORDINATE SYSTEM\n",
    "    offsets = np.min(coords_ras, axis=0)\n",
    "    print(offsets)\n",
    "    x_offset = offsets[0]\n",
    "    y_offset = offsets[1]\n",
    "    \n",
    "    # TRANSLATION VECTOR\n",
    "    offset_matrix = translation_matrix(x_offset, y_offset)\n",
    "\n",
    "    coords_shp_proc_bl = np.vstack((coords_shp[:, 0] - x_offset, coords_shp[:, 1] - y_offset)).T\n",
    "    coords_ras_proc_bl = np.vstack((coords_ras[:, 0] - x_offset, coords_ras[:, 1] - y_offset)).T\n",
    "    \n",
    "    plt.scatter(coords_shp_proc_bl[:, 0], coords_shp_proc_bl[:, 1])\n",
    "    plt.scatter(coords_ras_proc_bl[:, 0], coords_ras_proc_bl[:, 1])\n",
    "    plt.show()\n",
    "    \n",
    "    # FAST SEARCH STRUCTURE\n",
    "    kdtree     = cKDTree(coords_ras_proc_bl)\n",
    "    \n",
    "    # ITERATIVE CLOSEST POINT\n",
    "    reprojected_points = []\n",
    "    compounded_homography = np.eye(3)\n",
    "    proc_points = coords_shp_proc_bl\n",
    "    \n",
    "    rotation=True\n",
    "    shear=False\n",
    "    perspective=False\n",
    "    \n",
    "    plot=False\n",
    "    \n",
    "    transforms = []\n",
    "    grades     = []\n",
    "    \n",
    "    # ITERATE\n",
    "    for i in tqdm(range(50), disable=True):\n",
    "        \n",
    "        _, nearest_indices = kdtree.query(proc_points)\n",
    "        to_points = np.array([coords_ras_proc_bl[idx] for idx in nearest_indices])\n",
    "        \n",
    "        # TAKE ADJUSTMENT STEP\n",
    "        new_homography = adjustStep_affine(proc_points, coords_ras_proc_bl, kdtree,\n",
    "                                        shear=shear, rotation=rotation, perspective=perspective)\n",
    "        \n",
    "        if plot:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.scatter(proc_points[:, 0], proc_points[:, 1])\n",
    "            ax.scatter(coords_ras_proc_bl[:, 0], coords_ras_proc_bl[:, 1])\n",
    "            ax.scatter(to_points[:, 0], to_points[:, 1])\n",
    "\n",
    "            for i in tqdm(range(proc_points.shape[0])):\n",
    "                # print(proc_points[i, 0], proc_points[i, 1], to_points[i, 0], to_points[i, 1])\n",
    "                plt.plot([proc_points[i, 0], to_points[i, 0]],\n",
    "                         [proc_points[i, 1], to_points[i, 1]], 'ko', linestyle=\"--\")\n",
    "            plt.show()\n",
    "        \n",
    "        transform = new_homography.copy()\n",
    "        \n",
    "        # APPLY TRANSFORM FROM ADJUSTMENT TO PROCESSING POINTS AND APPEND TO LIST\n",
    "        reprojected_points.append(applyTransform(transform, proc_points))\n",
    "        \n",
    "        proc_points = applyTransform(transform, proc_points)\n",
    "        if plot:\n",
    "            plt.scatter(proc_points[:, 0], proc_points[:, 1])\n",
    "            plt.scatter(coords_ras_proc_bl[:, 0], coords_ras_proc_bl[:, 1])\n",
    "            plt.scatter(to_points[:, 0], to_points[:, 1])\n",
    "            plt.show()\n",
    "            \n",
    "        # COMPOUND TRANSFORMATION\n",
    "        compounded_homography = compounded_homography @ transform\n",
    "        \n",
    "        transforms.append(compounded_homography)\n",
    "        \n",
    "        def gradeFit(pts1, kdtree):\n",
    "            dist, _ = kdtree.query(pts1)\n",
    "            return np.sqrt(np.sum(dist ** 2))\n",
    "        \n",
    "        grades.append(gradeFit(proc_points, kdtree))\n",
    "        \n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            scale  = np.sqrt((new_homography[0, 0] ** 2 + new_homography[1, 1] ** 2) / 2)\n",
    "            offset = np.sqrt((new_homography[1, 2] ** 2 + new_homography[0, 2] ** 2) / 2)\n",
    "            # print(f\"Scale: {scale:.2f} Offset: {offset:.2f}\")\n",
    "    \n",
    "    best_transform = transforms[np.argmin(grades)]\n",
    "    best_points    = reprojected_points[np.argmin(grades)]\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(range(len(grades)), grades)\n",
    "        plt.scatter(np.argmin(grades), grades[np.argmin(grades)])\n",
    "        plotICP(reprojected_points, plot_skip=10, )\n",
    "        plt.show()\n",
    "    \n",
    "    # REVERSE Y AXIS\n",
    "    rev_y_axis = np.array([[1, 0, 0],\n",
    "                           [0,-1, 0],\n",
    "                           [0, 0, 1]])\n",
    "\n",
    "    # move = original_homography @ np.array([0, image_t.shape[0], 0])\n",
    "    translation = np.eye(3)\n",
    "    translation[1, 2] = image_t.shape[0]\n",
    "    \n",
    "    # adjustment = asymetric_adjustment @ symetric_adjustment\n",
    "    adjustment =  np.linalg.inv(best_transform.copy())\n",
    "\n",
    "    # rev_adj = rev_y_axis @ adjustment\n",
    "    rev_adj = adjustment.copy()\n",
    "    rev_adj[1, 1] = rev_adj[1, 1] * -1\n",
    "    \n",
    "    output_transform = original_homography @ translation @ rev_adj\n",
    "    # output_transform[1, 1] = output_transform[1, 1] * -1\n",
    "    offsets = output_transform @ np.array([[0, 0, 1], [image.shape[0], 0, 1]]).T\n",
    "    print(offsets)\n",
    "    offsets = offsets[:, 1] - offsets[:, 0]\n",
    "    # output_transform[0, 1] = -1 * output_transform[0, 1]\n",
    "\n",
    "    print(output_transform)\n",
    "    output_affine = Affine(*output_transform.flatten()[:6])\n",
    "    write_world_file_from_affine(output_affine, get_world_file_path(output_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef513c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(best_points[:, 0], best_points[:, 1], s=1)\n",
    "plt.scatter(coords_ras_proc_bl[:, 0], coords_ras_proc_bl[:, 1], s=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
